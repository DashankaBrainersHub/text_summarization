{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Textsummary with t5 \n",
    "We will try out, how good the pretrained t5 model handels German text summarization. For this we will use the huggingface transfomer library and a dataset, which contains German wikipedia pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "import tensorflow_datasets as tfds\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "SHUFFEL_SIZE = 1024\n",
    "\n",
    "learning_rate = 3e-5\n",
    "\n",
    "model_size = \"t5-base\"\n",
    "\n",
    "MAX_ARTICLE_LEN = 512\n",
    "\n",
    "MAX_HIGHLIGHT_LEN = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_size)\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_size)\n",
    "\n",
    "task_specific_params = model.config.task_specific_params\n",
    "if task_specific_params is not None:\n",
    "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
    "    \n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_t5for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "shared (TFSharedEmbeddings)  multiple                  24674304  \n",
      "_________________________________________________________________\n",
      "encoder (TFT5MainLayer)      multiple                  84954240  \n",
      "_________________________________________________________________\n",
      "decoder (TFT5MainLayer)      multiple                  113275392 \n",
      "=================================================================\n",
      "Total params: 222,903,936\n",
      "Trainable params: 222,903,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08, clipnorm=1.0)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe29c3875b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_file = \"../models/ckpt/checkpoint.ckpt\"\n",
    "model.load_weights(ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "We will try out a german Dataset which is an extract from the german Wikipedia Page. The first section is used for the summaray which should be predicted by the rest of the article. \n",
    "### References:\n",
    "- Fecht, Pascal, Sebastian Blank, and Hans-Peter Zorn. \"Sequential Transfer Learning in NLP for German Text Summarization.\" (2019).\n",
    "- https://drive.switch.ch/index.php/s/YoyW9S8yml7wVhN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data\"\n",
    "\n",
    "def get_tfrecord_dataset(drive_path, file_name):\n",
    "    features = {\n",
    "        'x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN], tf.int64),\n",
    "        'x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN], tf.int64),\n",
    "        'y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "        'y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
    "        'y_labels': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
    "    }\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(f\"{drive_path}/{file_name}.tfrecord\")\n",
    "\n",
    "    # Taken from the TensorFlow models repository: https://github.com/tensorflow/models/blob/befbe0f9fe02d6bc1efb1c462689d069dae23af1/official/nlp/bert/input_pipeline.py#L24\n",
    "    def decode_record(record, features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.io.parse_single_example(record, features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "\n",
    "    def select_data_from_record(record):\n",
    "        return record['x'], record['x_mask'], record['y'], record['y_ids'], record['y_labels']\n",
    "\n",
    "\n",
    "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
    "    dataset = dataset.map(select_data_from_record)\n",
    "    dataset = dataset.shuffle(100)\n",
    "    return dataset.batch(BATCH_SIZE)\n",
    "\n",
    "train_ds = get_tfrecord_dataset(path, \"train_cnn_daily_mail\")\n",
    "train_ds.prefetch(1024)\n",
    "\n",
    "val_ds = get_tfrecord_dataset(path, \"val_cnn_daily_mail\")\n",
    "test_ds = get_tfrecord_dataset(path, \"test_cnn_daily_mail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Train and Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_ids, input_mask, y):\n",
    "    # https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py\n",
    "    y_ids = y[:, :-1]\n",
    "    lm_labels = tf.identity(y[:, 1:])\n",
    "    lm_labels = tf.where(tf.equal(y[:, 1:],pad_token_id), -100, lm_labels)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # prediction_scores: (bs, 150, 32128)\n",
    "        # decoder_past_key_value_states: (bs, 512, 512), (bs, 8, 150, 64)\n",
    "        # z: (bs, 512, 512)\n",
    "        predictions, _, _ = model(input_ids, attention_mask=input_mask, decoder_input_ids=y_ids, lm_labels=lm_labels, training=True)\n",
    "        loss = loss_object(y[:, 1:], predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(y[:, 1:], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(input_ids, input_mask, y):\n",
    "    # https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py\n",
    "    y_ids = y[:, :-1]\n",
    "    lm_labels = tf.identity(y[:, 1:])\n",
    "    lm_labels = tf.where(tf.equal(y[:, 1:],pad_token_id), -100, lm_labels)\n",
    "    \n",
    "    predictions, _, _ = model(input_ids, attention_mask=input_mask, decoder_input_ids=y_ids, lm_labels=lm_labels, training=False)\n",
    "    v_loss = loss_object(y[:, 1:], predictions)\n",
    "\n",
    "    val_loss(v_loss)\n",
    "    val_accuracy(y[:, 1:], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-8074290c5563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m         \u001B[0;31m# training\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "log_interval = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    # reset metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    \n",
    "    val_batches = iter(train_ds)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, (input_ids, input_mask, y) in enumerate(train_ds):\n",
    "        # training\n",
    "        train_step(input_ids, input_mask, y)\n",
    "        \n",
    "        # validation\n",
    "        if i % log_interval == 0:\n",
    "            x_val, x_mask_val, y_val = next(val_batches)\n",
    "            val_step(x_val, x_mask_val, y_val)\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | [{:5d}/{:5d}] | '\n",
    "                  'ms/batch {:5.2f} | '\n",
    "                  'train acc {:5.2f} | val acc {:5.2f} |'\n",
    "                  'loss {:5.2f} | val loss {:5.2f}'.format(\n",
    "                    epoch, i, int(len_train/BATCH_SIZE),\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    train_accuracy.result() * 100, val_accuracy.result() * 100, \n",
    "                    train_loss.result(),  val_loss.result()))\n",
    "            start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Define Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from rouge_score import scoring\n",
    "\n",
    "class RougeScore:\n",
    "    '''\n",
    "    mostly from https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/evaluation/metrics.py \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, score_keys=None)-> None:\n",
    "        super().__init__()\n",
    "        if score_keys is None:  \n",
    "            self.score_keys = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "        \n",
    "        self.scorer = rouge_scorer.RougeScorer(self.score_keys)\n",
    "        self.aggregator = scoring.BootstrapAggregator()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def prepare_summary(summary):\n",
    "            # Make sure the summary is not bytes-type\n",
    "            # Add newlines between sentences so that rougeLsum is computed correctly.\n",
    "            summary = summary.replace(\" . \", \" .\\n\")\n",
    "            return summary\n",
    "    \n",
    "    def __call__(self, target, prediction):\n",
    "        \"\"\"Computes rouge score.''\n",
    "        Args:\n",
    "        targets: string\n",
    "        predictions: string\n",
    "        \"\"\"\n",
    "\n",
    "        target = self.prepare_summary(target)\n",
    "        prediction = self.prepare_summary(prediction)\n",
    "        \n",
    "        self.aggregator.add_scores(self.scorer.score(target=target, prediction=prediction))\n",
    "\n",
    "        return \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.rouge_list = []\n",
    "\n",
    "    def result(self):\n",
    "        result = self.aggregator.aggregate()\n",
    "        \n",
    "        for key in self.score_keys:\n",
    "            score_text = \"%s = %.2f, 95%% confidence [%.2f, %.2f]\"%(\n",
    "                key,\n",
    "                result[key].mid.fmeasure*100,\n",
    "                result[key].low.fmeasure*100,\n",
    "                result[key].high.fmeasure*100\n",
    "            )\n",
    "            print(score_text)\n",
    "        \n",
    "        return {key: result[key].mid.fmeasure*100 for key in self.score_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : time genreate batch: 4.1929614543914795\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "rouge_score = RougeScore()\n",
    "start_time = time.time()\n",
    "\n",
    "for i, (input_ids, input_mask, y, y_ids, y_labels) in enumerate(test_ds):   \n",
    "    summaries = model.generate(input_ids=input_ids, attention_mask=input_mask, max_length=150, early_stopping=True)\n",
    "\n",
    "    pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
    "    real = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in y]\n",
    "    \n",
    "    for pred_sent, real_sent in zip(pred, real):\n",
    "        rouge_score(pred_sent, real_sent)\n",
    "        predictions.append(str(\"pred sentence: \" + pred_sent + \"\\n\\nreal sentence: \" + real_sent))\n",
    "    \n",
    "    if (i % 10) == 0:\n",
    "        elapsed = (time.time() - start_time) / 10\n",
    "        print(i,\": time genreate batch:\", elapsed)\n",
    "        start_time = time.time()\n",
    "    if i > 51:\n",
    "        # otherwise it will take ages\n",
    "        break\n",
    "\n",
    "\n",
    "rouge_score.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Predict some Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pred in predictions[:10]:\n",
    "    print(\"------\")\n",
    "    print(pred)\n",
    "    print(\"------\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"../data/t5base_result_german.txt\"\n",
    "open(result_path, \"w\")\n",
    "for pred in predictions:\n",
    "    with open(result_path, \"a\") as file:\n",
    "        file.write(pred + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummary",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}