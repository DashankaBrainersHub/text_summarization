{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from my_sentence_piecer import MySentencePiecer\n",
    "from albert_pre import AlbertPre\n",
    "from tf_to_csv import TfToCsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'GeForce RTX 2080 Ti')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device(), torch.cuda.get_device_name(device=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENT_N = 30\n",
    "\n",
    "MAX_WORD_N = 150\n",
    "\n",
    "MAX_WORD_SENT_N = 300\n",
    "\n",
    "BATCHSIZE = 20\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sentence Piecer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence_piecer = MySentencePiecer(vocab_size=10000, force_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['<unk>', '<s>', '</s>', '▁the', 's', ',', '.', '▁to', '▁a', '▁in', '▁of', '▁and', '▁.', \"'\", '-', '▁was', '▁for', '▁on', '▁is', '▁he']\n",
      "[1459, 118, 5, 46, 13, 74, 1111, 6, 57, 18, 220, 1100, 4, 6, 2]\n",
      " hallo, i'm leaving. this is another sentences.</s>\n"
     ]
    }
   ],
   "source": [
    "print(sentence_piecer.vocab_size)\n",
    "print(sentence_piecer.vocab_list[:20])\n",
    "test = \"hallo, i'm leaving. this is another sentences.\"\n",
    "tokens = sentence_piecer.get_ids_from_vocab(test)\n",
    "print(tokens)\n",
    "print(sentence_piecer.get_real_text_from_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_piecer.vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "albert_pre = AlbertPre()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, article, n_highlights, highlights,transform=None):\n",
    "        self.x = self.to_tensor_list(article, dtype=torch.float)\n",
    "\n",
    "        self.y_n = torch.tensor(n_highlights, dtype=torch.long)\n",
    "        self.y = self.to_tensor_list(highlights, dtype=torch.long, pad=MAX_WORD_N)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y_n = self.y_n[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return x, y_n, y\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor_list(x, dtype, pad=None):\n",
    "\n",
    "        if pad is None:\n",
    "            tensor_list = [torch.tensor(x_i, dtype=dtype) for x_i in x]\n",
    "        else:\n",
    "            tensor_list = [torch.cat((torch.tensor(x_i[:MAX_WORD_N], dtype=dtype), \\\n",
    "                                      torch.zeros(pad - x_i[:MAX_WORD_N].shape[0], dtype=dtype))) for x_i in x]\n",
    "\n",
    "        return tensor_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_torch_dataset(name):\n",
    "    x, x_n, y_n,y = albert_pre.load_np_files(name)\n",
    "    return MyDataset(x,y_n,y)\n",
    "\n",
    "test_ds = load_torch_dataset(\"test\")\n",
    "train_ds = load_torch_dataset(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCHSIZE = 10\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCHSIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ContextDecoder(nn.Module):\n",
    "    def __init__(self, max_sent, d_model, nhead, dim_feedforward, out_dim=150, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.max_sent = max_sent\n",
    "        transfrom_decode_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward,\\\n",
    "                                                            dropout=dropout, activation='relu')\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(transfrom_decode_layer, num_layers=1)\n",
    "        self.out_put_layer = nn.Linear(3072, out_dim*200)\n",
    "\n",
    "\n",
    "    def forward(self, context, mask=None):\n",
    "        # dims\n",
    "        bs = context.shape[0]\n",
    "        dim_context = context.shape[2]\n",
    "\n",
    "        context_memory = torch.zeros(context[:,0,:].shape).to(device).reshape(bs,1,dim_context)\n",
    "\n",
    "        for i in range(self.max_sent):\n",
    "            context_memory = self.transformer_decoder(context[:,i,:].reshape(bs,1,dim_context), context_memory)\n",
    "\n",
    "        # reshape\n",
    "        context_memory = context_memory.reshape(bs, dim_context)\n",
    "        out = self.out_put_layer(context_memory).reshape(150,-1,200)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_vocab, emsize, nhead, nhid, nlayers, max_sent=30, c_d_model=3072, dropout=0.2):\n",
    "        \"\"\"\n",
    "        @param n_vocab: vocab_size\n",
    "        @param emsize: embedding size\n",
    "        @param nhead: the number of heads in the multiheadattention models\n",
    "        @param nhid: the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        @param nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        @param dropout: the dropout value\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(emsize, dropout)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout)\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(n_vocab, emsize)\n",
    "        self.emsize = emsize\n",
    "        self.decoder = nn.Linear(emsize, n_vocab)\n",
    "        self.context_decoder = ContextDecoder(max_sent, c_d_model, nhead, nhid, dropout=dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "    def predict_one(self, context, n):\n",
    "        context_sum = self.context_decoder(context)\n",
    "        in_src = []\n",
    "\n",
    "        for i in range(torch.max(n)):\n",
    "            if i == 0:\n",
    "                in_tokens = torch.ones((MAX_WORD_N, 1), dtype=torch.long).to(device)\n",
    "            else:\n",
    "                zeros = torch.ones(((MAX_WORD_N-i), 1), dtype=torch.long).to(device)\n",
    "                tokens = torch.LongTensor(in_src).view(-1,1).to(device)\n",
    "                in_tokens = torch.cat((tokens, zeros), dim=0)\n",
    "            src = self.encoder(in_tokens) * math.sqrt(self.emsize)\n",
    "            src = self.pos_encoder(src)\n",
    "           \n",
    "            output = self.transformer_encoder(src, self.src_mask)\n",
    "            output += context_sum\n",
    "            output = self.decoder(output)\n",
    "            out_token = output.argmax(2)\n",
    "            out_token = out_token[i].item()\n",
    "            in_src.append(out_token)\n",
    "\n",
    "        return in_src\n",
    "\n",
    "\n",
    "    def forward(self, context, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.emsize)\n",
    "        src = self.pos_encoder(src)\n",
    "           \n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        context_sum = self.context_decoder(context)\n",
    "        \n",
    "        output += context_sum\n",
    "        output = self.decoder(output)\n",
    "#         print(\"output\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_vocab = sentence_piecer.vocab_size\n",
    "model = TransformerModel(n_vocab=n_vocab, emsize=200, nhead=2, nhid=200,\\\n",
    "                         nlayers=1, max_sent=30, c_d_model=3072, dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_sent = iter(test_loader)\n",
    "x_test, n_test, y_test =  next(test_sent)\n",
    "\n",
    "x_test = x_test[0,:,:].view(1,30,3072).to(device)\n",
    "n_test = n_test[0].to(device)\n",
    "y_test = y_test[0,:].view(1,150).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30, 3072]), tensor(43, device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_sentence = sentence_piecer.get_real_text_from_ids(y_test.view(-1)[:n_test.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " experts question if packed out planes are putting passengers at risk . u.s consumer advisory group says minimum space must be stipulated . safety tests conducted on planes with more leg room than airlines offer .</s>\n",
      "Pred Sent:   account re shark debt shark beaten appointed shark satisfied shark satisfied account teenager verbal attend appointed documentary compwilfried shark devoted muslims teenager morning teenager jail command command command sharkwrittenfra concealabove command information informationfrafra ambitious stability rob want\n"
     ]
    }
   ],
   "source": [
    "def evaluate(eval_model, test_loader):\n",
    "    eval_model.eval()\n",
    "    test_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, n, y) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            n = n.to(device)\n",
    "            y = y.permute(1,0).to(device)\n",
    "\n",
    "            output = eval_model(x, y)\n",
    "            loss = criterion(output.view(MAX_WORD_N, n_vocab, -1), y)\n",
    "            test_loss.append(loss.item())\n",
    "            if i > 5:\n",
    "                break\n",
    "\n",
    "        sent_ids = eval_model.predict_one(x_test, n_test)\n",
    "        pred_sentence = sentence_piecer.get_real_text_from_ids(sent_ids)\n",
    "\n",
    "#     print(\"REAL Sent: \", real_sentence)\n",
    "    print(\"Pred Sent: \", pred_sentence)\n",
    "\n",
    "    test_loss = np.array(test_loss)\n",
    "    return np.mean(test_loss)\n",
    "print(real_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Sent:   its me wantmmer a a a a a a them all a land a a a a a a \"\"he a a a a a a a a world me a a a a a a a aalhe a\n",
      "| epoch   0 | [  200/ 1337] | lr 5.00 | ms/batch 219.57 | loss  6.28 | val loss  9.54 | ppl   533.92\n",
      "Pred Sent:  al justa tax tax tax tax tax inquiry taxalal taxa tax tax tax tax inquiry inquiryalna tax tax tax tax tax inquiry taxalaaa00 tax inquiry inquiry inquiry inquiryala tax\n",
      "| epoch   0 | [  400/ 1337] | lr 5.00 | ms/batch 213.96 | loss  5.23 | val loss  4.33 | ppl   186.01\n",
      "Pred Sent:   this mark mark mark in in bonus in in in one have in in in in in in in in have wife in in in in in in in in them in in in in in in in in in. in in\n",
      "| epoch   0 | [  600/ 1337] | lr 5.00 | ms/batch 214.32 | loss  4.70 | val loss  4.79 | ppl   109.59\n",
      "Pred Sent:   a a a tax a tax to to tax tax a a a a to to to tax tax to a a a to to to to to to tax a a a a to to to to to to a a a\n",
      "| epoch   0 | [  800/ 1337] | lr 5.00 | ms/batch 217.04 | loss  4.33 | val loss  3.73 | ppl    76.14\n",
      "Pred Sent:   in do do setting individuals martinez martinez martinez martinez martinez in do individuals individuals individuals individuals setting individuals shah rush inling setting supreme setting martinez setting martinez shah martinez in do do rush rush martinez shah shah shah shah in do do\n",
      "| epoch   0 | [ 1000/ 1337] | lr 5.00 | ms/batch 214.63 | loss  4.19 | val loss  3.94 | ppl    65.76\n",
      "Pred Sent:  </s></s> in site want however however custody however however</s></s></s> cook custody$ however however however however</s></s></s> site however custody however however however however \"\"</s></s> site$ custody however custody however however<s></s> site\n",
      "| epoch   0 | [ 1200/ 1337] | lr 5.00 | ms/batch 215.27 | loss  4.18 | val loss  3.86 | ppl    65.46\n",
      "Pred Sent:  s chemical cook returning cook cook returning cookread returnings approach cook cook commission commissionread returningreadreads commission cook planes returning returning cook returning chemicalreads commission commission commission returning cook cook planesreadreads commission cook\n",
      "| epoch   1 | [  200/ 1337] | lr 5.00 | ms/batch 231.76 | loss  4.09 | val loss  3.87 | ppl    59.95\n",
      "Pred Sent:   to to to to to to to to tos. to to to to to to to tos. to to to to to to toss. to to to tosssss. to to\n",
      "| epoch   1 | [  400/ 1337] | lr 5.00 | ms/batch 229.94 | loss  4.03 | val loss  3.93 | ppl    56.02\n",
      "Pred Sent:   not june bonus bonus aid bonus bonus bonus bonus bonus not tax bonus bonus bonus bonus bonus bonus bonus bonus being mark bonus bonus bonus bonus bonus bonus bonus bonus them mark breach bonus bonus bonus bonus bonus bonus bonus being breach bonus\n",
      "| epoch   1 | [  600/ 1337] | lr 5.00 | ms/batch 216.34 | loss  4.00 | val loss  3.66 | ppl    54.43\n",
      "Pred Sent:   to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "| epoch   1 | [  800/ 1337] | lr 5.00 | ms/batch 230.10 | loss  3.94 | val loss  3.71 | ppl    51.22\n",
      "Pred Sent:   has do do do do do public do public public an do do ago three ago ago ago ago ago world do do ago ago do ago ago ago ago, do do ago ago ago ago ago ago ago, do do\n",
      "| epoch   1 | [ 1000/ 1337] | lr 5.00 | ms/batch 225.02 | loss  3.90 | val loss  3.68 | ppl    49.58\n",
      "Pred Sent:  </s></s></s></s></s> custody custody custody tiny custody</s></s></s></s></s> -- investigating joke custody investigating</s></s></s></s></s></s> investigating investigating investigating joke</s></s></s></s></s></s> however joke joke however</s></s></s>\n",
      "| epoch   1 | [ 1200/ 1337] | lr 5.00 | ms/batch 226.57 | loss  3.89 | val loss  3.64 | ppl    49.11\n",
      "Pred Sent:   has site want want want want want want lawsuitreads want represent £9 session rowreadvivvivreads to to to to cy row cy materials materialss to to to to to t materialsreadreads to to\n",
      "| epoch   2 | [  200/ 1337] | lr 5.00 | ms/batch 226.72 | loss  3.94 | val loss  3.61 | ppl    51.33\n",
      "Pred Sent:   been to to to to to to to to to not tax to to to to to to to to back to to to to to to to to to not to to to to to to to to to. to to\n",
      "| epoch   2 | [  400/ 1337] | lr 5.00 | ms/batch 229.59 | loss  3.88 | val loss  3.67 | ppl    48.31\n",
      "Pred Sent:   fromiya bonus bonus bonus bonus bonus bonus bonus bonus being bonus bonus bonus bonus bonus bonus bonus bonus bonus being mark bonus bonus bonus bonus bonus bonus bonus bonus being bonus bonus bonus bonus bonus bonus bonus bonus bonus being inter bonus\n",
      "| epoch   2 | [  600/ 1337] | lr 5.00 | ms/batch 228.67 | loss  3.89 | val loss  3.61 | ppl    48.95\n",
      "Pred Sent:   to a a to to to to to to to to a a to to to to to to to a a a to to to to to to to to a to a to to to to to to a a a\n",
      "| epoch   2 | [  800/ 1337] | lr 5.00 | ms/batch 228.59 | loss  3.82 | val loss  3.64 | ppl    45.70\n",
      "Pred Sent:   have 0 0 0 0 0 0 0 0 0, 0 0 0 0 individuals individuals individuals individuals individuals,etmart awards foul foul joke foul foul foul, 0 0 0 0 0 foul foul association joke, 0 0\n",
      "| epoch   2 | [ 1000/ 1337] | lr 5.00 | ms/batch 227.51 | loss  3.82 | val loss  3.59 | ppl    45.51\n",
      "Pred Sent:  </s></s></s></s></s></s></s></s></s> cook</s></s></s></s></s></s></s> cook cook cook</s></s></s></s></s></s> cook cook joke joke</s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "| epoch   2 | [ 1200/ 1337] | lr 5.00 | ms/batch 229.48 | loss  3.81 | val loss  3.55 | ppl    45.17\n",
      "Pred Sent:   to yu yu want officer want want singleusussidid want single want wantusviv $4sidvivvivususususus $4sid approach andersonusus t single $4 tsidmart\n",
      "| epoch   3 | [  200/ 1337] | lr 5.00 | ms/batch 229.70 | loss  3.87 | val loss  3.65 | ppl    47.77\n",
      "Pred Sent:  . broken 75 0 0 0 0 arena arena arena. 0... promise university arena arena arena.et variety...gor datinggor arena...... arena arena arena arena...\n",
      "| epoch   3 | [  400/ 1337] | lr 5.00 | ms/batch 225.48 | loss  3.77 | val loss  3.55 | ppl    43.59\n",
      "Pred Sent:   has anna bonus bonus bonus bonus bonus bonus bonus bonus being bonus bonus bonus bonus bonus bonus bonus bonus bonus beinget bonus bonus bonus bonus bonus bonus bonus bonus beinget bonus bonus bonus original original original bonus bonus being anna anna\n",
      "| epoch   3 | [  600/ 1337] | lr 5.00 | ms/batch 229.45 | loss  3.81 | val loss  3.60 | ppl    44.96\n",
      "Pred Sent:   to to to to to to to to to to to to to to a to to to to to a a to to to to a a to to to to to to to to to to to to to to to\n",
      "| epoch   3 | [  800/ 1337] | lr 5.00 | ms/batch 230.72 | loss  3.78 | val loss  3.60 | ppl    43.87\n",
      "Pred Sent:   havepark 75 screaming 0 0 0 hang original appealing, 0 0 0 running running filming joke refugee running,idlevel screaming liga running joke joke screaming joke,etlevel original original original original original original shah, 0mart\n",
      "| epoch   3 | [ 1000/ 1337] | lr 5.00 | ms/batch 230.32 | loss  3.74 | val loss  3.59 | ppl    42.29\n",
      "Pred Sent:  </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "| epoch   3 | [ 1200/ 1337] | lr 5.00 | ms/batch 229.39 | loss  3.74 | val loss  3.53 | ppl    41.95\n",
      "Pred Sent:   from aguero 75airvivvivvivvivvivviv \"\" 0 farm deserve sessionvivhmvivvivviv them commission commission commission commission commission commission brisbane brisbane brisbanes commission commission commission commission commission commission commission commission brisbanes commission commission\n",
      "| epoch   4 | [  200/ 1337] | lr 5.00 | ms/batch 234.09 | loss  3.79 | val loss  3.53 | ppl    44.26\n",
      "Pred Sent:   to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "| epoch   4 | [  400/ 1337] | lr 5.00 | ms/batch 224.56 | loss  3.70 | val loss  3.57 | ppl    40.32\n",
      "Pred Sent:   not june 75 original original original madison students chemical aid being scoring scoring original 1981shirt 1981 originalshirtshirt being scoring scoring network leak soldshirtshirtshirtshirt beinget scoring original original original original original original 1981 being irish scoring\n",
      "| epoch   4 | [  600/ 1337] | lr 5.00 | ms/batch 230.57 | loss  3.75 | val loss  3.55 | ppl    42.45\n",
      "Pred Sent:   a a a to to to appealing to to appealing to a to to to to to to to to to to to to to to to to to to to a a to to to to to to to to to to\n",
      "| epoch   4 | [  800/ 1337] | lr 5.00 | ms/batch 229.31 | loss  3.69 | val loss  3.60 | ppl    39.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Sent:  <s> appealinglevel appealing appealing appealing appealing appealing appealing appealing<s> 0level explain appealing appealinghm debris refugee appealing<s>idlevel appealing appealing appealing appealing appealing appealing appealing, appealinglevel appealing appealing setting original original expire appealing<s> classesmart\n",
      "| epoch   4 | [ 1000/ 1337] | lr 5.00 | ms/batch 229.39 | loss  3.67 | val loss  3.53 | ppl    39.17\n",
      "Pred Sent:  </s> visiting</s></s></s></s></s> joke joke joke</s></s></s></s></s></s>hm joke joke joke</s></s></s></s></s></s> joke joke joke joke</s></s></s></s></s></s></s> joke joke joke</s></s></s>\n",
      "| epoch   4 | [ 1200/ 1337] | lr 5.00 | ms/batch 228.97 | loss  3.68 | val loss  3.53 | ppl    39.84\n",
      "Pred Sent:   has aguero southeastairvivvivviv lambvivvivs 0 commission pipeline novakvivreadvivvivvivs commissionviv pipeline sessionviv novakviv session sessionset poppy commission commission appealing novak immune studio appealings commissionlevel\n",
      "| epoch   5 | [  200/ 1337] | lr 5.00 | ms/batch 233.15 | loss  3.74 | val loss  3.56 | ppl    42.22\n",
      "Pred Sent:   to promise 75 to to to to to to to to to to to to to to to to to back to to to to to to to to to to to to to to to to to to tos to to\n",
      "| epoch   5 | [  400/ 1337] | lr 5.00 | ms/batch 229.07 | loss  3.63 | val loss  3.55 | ppl    37.87\n",
      "Pred Sent:   from appealingularni 1981 appealing students partner original appealing being june scoring original 1981 1981 1981 original 1981 1981 being scoringviv 1981 disability maintenance maintenance maintenance cou cou beinget in originalgra in original inshirt particularly a in in\n",
      "| epoch   5 | [  600/ 1337] | lr 5.00 | ms/batch 229.99 | loss  3.68 | val loss  3.56 | ppl    39.48\n",
      "Pred Sent:   to to to to to to a a<s><s> a a a a a a a a a a a a a a a a a a a<s> a a a a a a a<s><s><s> a a a\n",
      "| epoch   5 | [  800/ 1337] | lr 5.00 | ms/batch 230.54 | loss  3.61 | val loss  3.58 | ppl    37.14\n",
      "Pred Sent:   in appealing 75 lit session appealing appealing appealing appealing appealing, 0 activist ebay appealing ebay ebay ebay individuals individuals backetviv sure carriage spectator appealing spectator running appealing backet cemetery appealing appealing appealing gorilla gorilla gorilla appealing, traditionalmart\n",
      "| epoch   5 | [ 1000/ 1337] | lr 5.00 | ms/batch 230.31 | loss  3.62 | val loss  3.52 | ppl    37.22\n",
      "Pred Sent:   her defect 75air hunger 0 tiny appealing patients appealing</s>ssss novakey joke seen seen</s> mark emails rack rack eric keep affiliatehouse seend tomorrow rackill commission again original again emails again</s> commission emails\n",
      "| epoch   5 | [ 1200/ 1337] | lr 5.00 | ms/batch 230.29 | loss  3.62 | val loss  3.50 | ppl    37.38\n",
      "Pred Sent:   has aguero 75air’.’. want’. chemical’. so 0 barcelona pay’. poppy want want chemical want \"\" mark poppy’. want want want chemical chemical chemical \"\"’. poppy’. qpr chemical chemical chemical chemical chemical before irish broken\n",
      "| epoch   6 | [  200/ 1337] | lr 5.00 | ms/batch 233.68 | loss  3.69 | val loss  3.52 | ppl    40.18\n",
      "Pred Sent:   to to to to to to to to to to to to to to to to to to to to to to toim to to to to to to to to to to to to to to to to to association to\n",
      "| epoch   6 | [  400/ 1337] | lr 5.00 | ms/batch 233.44 | loss  3.58 | val loss  3.57 | ppl    35.99\n",
      "Pred Sent:   not juneular original accessible town filming er original asking being june without certainly 1981 certainly certainly breach 1981 additional being june poppy network fbi 1981 1981 cou images cou being appealing certainlyillss ers immune weren a associations\n",
      "| epoch   6 | [  600/ 1337] | lr 5.00 | ms/batch 237.29 | loss  3.62 | val loss  3.59 | ppl    37.25\n",
      "Pred Sent:   to appealing 75air asking appealing appealing appealing65 appealing to author ambassador appealing a author65 cautious cautiousgra a participate expand to shar short keep short short appealing to research again appealing appealing cautious cautious cautious cautious appealing. a a\n",
      "| epoch   6 | [  800/ 1337] | lr 5.00 | ms/batch 238.85 | loss  3.55 | val loss  3.59 | ppl    34.87\n",
      "Pred Sent:   her june 75 ray ray again appealing appealing explain appealing world author incident explain deemed appealing deemed spectator spectator runningdvivviv if carriage spectator spectator spectator spectator appealing world appealing again appealing decline appealing appealing spectator appealing appealing being appealing users\n",
      "| epoch   6 | [ 1000/ 1337] | lr 5.00 | ms/batch 226.01 | loss  3.55 | val loss  3.61 | ppl    34.66\n",
      "Pred Sent:   her activist cemetery lit questmp tiny humanhousehouse</s> author activist</s></s> investigationhm patients israel israel</s> research penaltiesleg</s></s> placed gorilla running rodgers</s> research however</s></s></s></s></s> point again</s></s></s>\n",
      "| epoch   6 | [ 1200/ 1337] | lr 5.00 | ms/batch 223.25 | loss  3.56 | val loss  3.59 | ppl    35.03\n",
      "Pred Sent:   has greet 75 litki lambert want barcelona $4 want \"\" want touched hiddenmen authentichm want $4 $4 \"\" june approach invented qpr officer officer novak chemical novak \"\" defending approach opportunities approach approachhm novak novak wood them dozen qatar\n",
      "| epoch   7 | [  200/ 1337] | lr 5.00 | ms/batch 223.44 | loss  3.65 | val loss  3.63 | ppl    38.56\n",
      "Pred Sent:   to promise division lit duty to to to to classes keep to to to to togor missing togor will to varietyim classes carriage keep datingimfriend<s> susan data to to to to immune immune immune<s> classes to\n",
      "| epoch   7 | [  400/ 1337] | lr 5.00 | ms/batch 219.93 | loss  3.52 | val loss  3.63 | ppl    33.86\n",
      "Pred Sent:   have anna name lit session m name mostly deeply asking being 0 cho cho being certainly certainly asking asking asking being asking scoring asking m m m session chemical md mostly name session session name gorilla joke gorilla werend irish certainly\n",
      "| epoch   7 | [  600/ 1337] | lr 5.00 | ms/batch 220.89 | loss  3.56 | val loss  3.70 | ppl    35.02\n",
      "Pred Sent:   has broken broken injection olivia appealing appealing olivia olivia appealing a author olivia appealing appealing appealing65 broken olivia appealing being appealing olivia appealing olivia again appealing olivia olivia appealing to appealing again appealing appealing appealing appealing appealing appealing appealing</s>, to\n",
      "| epoch   7 | [  800/ 1337] | lr 5.00 | ms/batch 219.78 | loss  3.50 | val loss  3.61 | ppl    33.01\n",
      "Pred Sent:   have june 75 lit individuals lambert ray christina boxing individuals back 0 lights explain freezing freezing65 individuals individuals individuals back participate aimctic decline christina carriage carriage decline carriage<s> participate decline original decline participate creating carriage decline appealing<s> tomorrow youtube\n",
      "| epoch   7 | [ 1000/ 1337] | lr 5.00 | ms/batch 220.06 | loss  3.52 | val loss  3.65 | ppl    33.77\n",
      "Pred Sent:  </s> episode seventh quest quest bundesliga tiny bundesligamother bundesliga</s> bundesliga questionmother quest eatenhouse penalties bundesligahoused research quest bundesliga morris mrshousehousehouse eaten</s> research eaten research quest bundesliga bundesliga eaten eaten eaten</s> commission</s>\n",
      "| epoch   7 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.79 | loss  3.51 | val loss  3.57 | ppl    33.36\n",
      "Pred Sent:   has defect 75air administer lambert experienced barcelona $4 want them 0 barcelona hidden er includehm barcelonavivng them qprviv directed qpr isolation barcelona steer chemical appealing \"\" appealing again representiestnghm 1972 self novak them based demonstration\n",
      "| epoch   8 | [  200/ 1337] | lr 5.00 | ms/batch 223.29 | loss  3.62 | val loss  3.61 | ppl    37.17\n",
      "Pred Sent:   but broken 75 lit keep chad keep keep chad chad keep chad lapose keep eatengor chad chad keepal vitamin smiling youngest keep keep keep keep keep keep butet data sliest arena quality novak immune novak office tomorrow users\n",
      "| epoch   8 | [  400/ 1337] | lr 5.00 | ms/batch 220.15 | loss  3.45 | val loss  3.63 | ppl    31.48\n",
      "Pred Sent:  ed propose nameair asking asking name barcelonaworldmal have june barcelona lit missing images being debt images images being asking mut lit head name mworld chemical imagesd research again releasedgra name er er chemical images they jamesett\n",
      "| epoch   8 | [  600/ 1337] | lr 5.00 | ms/batch 219.81 | loss  3.51 | val loss  3.70 | ppl    33.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Sent:   tobillion toair asking appealing appealing a a appealing a a a a a a65 a to users a a a to a to to a to appealing a a a write a a a wheat wheat appealing a a a\n",
      "| epoch   8 | [  800/ 1337] | lr 5.00 | ms/batch 220.11 | loss  3.42 | val loss  3.69 | ppl    30.64\n",
      "Pred Sent:  <unk> chemical 75 sex social hand appealing stuff chemical possible say breach ipswich pipelinequimore individuals individuals individuals running<s> rush dropped youngestrie running spectator spectator running rush have superintendent rush rush rush rush rush rush rushrie world merely users\n",
      "| epoch   8 | [ 1000/ 1337] | lr 5.00 | ms/batch 220.93 | loss  3.43 | val loss  3.75 | ppl    30.95\n",
      "Pred Sent:   her santiagorich lit quest hand tiny 3,000 boxing 3,000 ‘ 3,000 activist treat historian expirehouse joke slur nhs them dogs airwick fun eric upon uponari upond itself upon brand rack fun seeking jokewing fun</s> bloom ties\n",
      "| epoch   8 | [ 1200/ 1337] | lr 5.00 | ms/batch 220.43 | loss  3.45 | val loss  3.63 | ppl    31.59\n",
      "Pred Sent:   has defect wantair include lambert tiny para chemical wantd chad barcelona represent christian novakhm debt swoop usersd defectus lit qpr include barcelona include chemical novakd users lake tarus want lambert tar chemical td based surround\n",
      "| epoch   9 | [  200/ 1337] | lr 5.00 | ms/batch 223.05 | loss  3.59 | val loss  3.65 | ppl    36.25\n",
      "Pred Sent:  . neighborhood regarding lit raw chad keep classes chad chad have author interaction pin keep lewis65 users users users. vitamin dropped lit keep tear placed chad chemical chad them camp raw tear phase tear65 immune chemical users. classes ca\n",
      "| epoch   9 | [  400/ 1337] | lr 5.00 | ms/batch 220.09 | loss  3.40 | val loss  3.61 | ppl    29.94\n",
      "Pred Sent:   in annabat valentineshirt hand nfl penaltiesworld novak other scoring scoring novak novak novak novak novak novak users being participate fatal novak novak scoring placed novak novak novak other paul novak fbi novak think er novak include novak a scoring scoring\n",
      "| epoch   9 | [  600/ 1337] | lr 5.00 | ms/batch 219.82 | loss  3.44 | val loss  3.65 | ppl    31.12\n",
      "Pred Sent:   has tim singerair asking asking er er asking asking would author scoring phones er author65 asking asking usersd asking housing guards shar research placed confronted asking asking children research reachedairgra cautious er er er er aring raw\n",
      "| epoch   9 | [  800/ 1337] | lr 5.00 | ms/batch 219.52 | loss  3.37 | val loss  3.67 | ppl    29.03\n",
      "Pred Sent:   have report removing sex report himself report forest god report back so report explain objects novak novak novak users users back report swift sure qpr resistver spectator god carriage back dragon reportou unsuccessful trust report novak tre sarah in traditional users\n",
      "| epoch   9 | [ 1000/ 1337] | lr 5.00 | ms/batch 219.32 | loss  3.37 | val loss  3.72 | ppl    29.22\n",
      "Pred Sent:   herhouseins released properties lambert tiny 3,000 personality 3,000</s> rodgers activist rodgers objects ebay express debt slur usersd cover son youngest keep eric 3,000 3,000 alexandra lit \"\" tomorrow however released israelvery novak novak novak muchs traditional much\n",
      "| epoch   9 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.88 | loss  3.38 | val loss  3.67 | ppl    29.42\n",
      "Pred Sent:   has defectiest lit breast lambert tiny criticism filming chadring chad firing pay er exportread debt wheat er so defect fatal chile er export chile wheat chemical er will write data opportunitiesiest export er er self ers commission demonstration\n",
      "| epoch  10 | [  200/ 1337] | lr 5.00 | ms/batch 222.76 | loss  3.57 | val loss  3.65 | ppl    35.51\n",
      "Pred Sent:   but chad regarding tu asking chad chad canada chad chad will chad relatives chad missingmoregor chad depression running but vitamin alertedimofficial chad anderson chad chemical chad but anderson matter boys leak dance quality quality chemicalfriend but classes\"\n",
      "| epoch  10 | [  400/ 1337] | lr 5.00 | ms/batch 220.26 | loss  3.35 | val loss  3.64 | ppl    28.37\n",
      "Pred Sent:   hum neighborhood capital valentine unless aid attended town attend aid one closest often juan aid holds65 breach drunkgra have neighborhood sniff lit attractive aid m holds chemical aidate qualifier panel ourate mate aid aid aid ourdia stephen\n",
      "| epoch  10 | [  600/ 1337] | lr 5.00 | ms/batch 220.56 | loss  3.39 | val loss  3.77 | ppl    29.53\n",
      "Pred Sent:   to cautious bloom lit asking to lit to to to. objects bloom to objects to to to asking users to bloom to to to to to to to to to users toair to to to to to to. bloom to\n",
      "| epoch  10 | [  800/ 1337] | lr 5.00 | ms/batch 224.73 | loss  3.30 | val loss  3.77 | ppl    27.11\n",
      "Pred Sent:   hague appealing corpovic peninsula pen nflique extremist report world countries scoring warrior deemed includehmbillion users users being setting adult foundation franchise tampa setting franchise franchise franchise so diagnosed spending setting qpr paulactivate novak point president, egyptian placed\n",
      "| epoch  10 | [ 1000/ 1337] | lr 5.00 | ms/batch 223.72 | loss  3.32 | val loss  3.74 | ppl    27.56\n",
      "Pred Sent:   contain visitingrich lit friendly persistent 3,000 penaltiesmother 3,000 them 3,000 novak tipped belt novak korean joke slur novakd belt penalties youngest novak belt jones 3,000house beltd qualifier beltconsidertal swan belt novak beltrie themey belt\n",
      "| epoch  10 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.99 | loss  3.34 | val loss  3.74 | ppl    28.24\n",
      "Pred Sent:   has defectiest lit point lambert lit para filming novak them lit novak litiest novak novak novak novak novak its placing cro lit qprabsolutely novak novak novak novak will appealing pra writeiest was novak novak novak novak them flaw novak\n",
      "| epoch  11 | [  200/ 1337] | lr 5.00 | ms/batch 223.68 | loss  3.53 | val loss  3.67 | ppl    34.10\n",
      "Pred Sent:   chad chad regarding tomorrow asking chad keep keep chad missing keepver beginningver missing novak castle novak asking users has tomorrow dropped missing keep missing keep dating missing tear willpet users teargra 2012activate novak somehow novak have falcon novak\n",
      "| epoch  11 | [  400/ 1337] | lr 5.00 | ms/batch 220.40 | loss  3.29 | val loss  3.72 | ppl    26.76\n",
      "Pred Sent:  ed neighborhood nameair cuddle90 author town participate novak author authorque cuddle novak novak novak novak beef novak have novak author youngest novak m m novak novak author u research bloom novakgra 2012 users novak novak novak have james users\n",
      "| epoch  11 | [  600/ 1337] | lr 5.00 | ms/batch 220.44 | loss  3.33 | val loss  3.82 | ppl    28.05\n",
      "Pred Sent:   to cautiousiestair asking 2012 2012 double upon objects to objects decidinggeneral objects cautious cautiousbillion cautiousgrad wheat sharbound shar short placed confronted objects upon shot research writeair victim spoken scar scar scar scar a bloom affordable\n",
      "| epoch  11 | [  800/ 1337] | lr 5.00 | ms/batch 220.58 | loss  3.24 | val loss  3.81 | ppl    25.62\n",
      "Pred Sent:   chad rush removing platform peninsula ray ray 3,000 boxing 3,000 same releasedbillion runningcovered includehmbillion items users released users dropped lit novak tampa 3,000 3,000 users pond an users usersouou registeredhmbillionbillion users released containing users\n",
      "| epoch  11 | [ 1000/ 1337] | lr 5.00 | ms/batch 220.84 | loss  3.26 | val loss  3.82 | ppl    25.99\n",
      "Pred Sent:   contain visiting penalties lit representatives silent express penalties crime 3,000 – 3,000 roleverhouse representativeshouse joke slur users her cover reached investigating novak research certain norhouse lit</s> site moss releasedrie research</s> novak chemical scar</s> wool passage\n",
      "| epoch  11 | [ 1200/ 1337] | lr 5.00 | ms/batch 220.04 | loss  3.29 | val loss  3.78 | ppl    26.75\n",
      "Pred Sent:   ol defectiest lit point silent tiny criticism filming pension will ethiopia objects killed hague ebay want debt identificationng its bloom fatal youngest pavementabsolutely m investigatingx er will n retrieve holiday properties watched n based selfcoloured them based retired\n",
      "| epoch  12 | [  200/ 1337] | lr 5.00 | ms/batch 223.26 | loss  3.50 | val loss  3.72 | ppl    33.05\n",
      "Pred Sent:   chad chad regarding lit asking shi keep canada filming fire fire sub beginning immuneiest novakseason beginning asking usersver vitamin vitamin lit sub missing placed nor users carriage. vitamin users somehow carriage 2012 users novak users users. classes users\n",
      "| epoch  12 | [  400/ 1337] | lr 5.00 | ms/batch 220.61 | loss  3.24 | val loss  3.76 | ppl    25.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Sent:  ed hipbat valentine norway fbi lit penalties deeply novak<s> objects bad lit novak novak novak novak beef users world novak sniff investigating novak juan m 3,000 novak asking them romance write write took strategytan novak cuddle novak atan hal\n",
      "| epoch  12 | [  600/ 1337] | lr 5.00 | ms/batch 220.29 | loss  3.29 | val loss  3.84 | ppl    26.97\n",
      "Pred Sent:   short cautious bloomfriend point pen 09:4 double asking asking – objects deciding horrifying asking include cautious include asking asking have wheat cancellationping leak 09:4 leak bieber bieber appealing adent wheat wheat essential pen god somehow cautious to to traditional transfer\n",
      "| epoch  12 | [  800/ 1337] | lr 5.00 | ms/batch 220.56 | loss  3.19 | val loss  3.81 | ppl    24.17\n",
      "Pred Sent:  johnrichrich includezan lambert pleased treat totally slice being cautious carriageleg deemed include65billion items users \"\"shaw vitamin youngest qpr jaguar carriage include include carriage \"\" dragon usersourie watched users include include users<s> youngest users\n",
      "| epoch  12 | [ 1000/ 1337] | lr 5.00 | ms/batch 219.81 | loss  3.20 | val loss  3.87 | ppl    24.58\n",
      "Pred Sent:   heriestrich lit properties doubt express para slur 3,000 her rack activistever objects novak korean 4. slur doubt her tomorrow suddenly youngest without without without slur alexandra cyd cy however title cy 4.househouse eaten eaten</s> wool eaten\n",
      "| epoch  12 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.62 | loss  3.22 | val loss  3.80 | ppl    25.08\n",
      "Pred Sent:   has defectiestzan point hugo lit para faster chad \"\" chad faster represent faster marvel faster debt beef asking has defect suspicion lit abdullah ticket rica beef chemical cy \"\" susan retrieve represent cy fell caption novak cuddle novak has appealing retired\n",
      "| epoch  13 | [  200/ 1337] | lr 5.00 | ms/batch 223.59 | loss  3.45 | val loss  3.74 | ppl    31.51\n",
      "Pred Sent:   mean neighborhoodiest tim raw asking keep users users chad has author 1976 rawiest ebay chad debt depression users back tomorrow alerted maj keepkel dish datingpet carriage but susan users depression county complaintsactivate novak keep novak have alertedgor\n",
      "| epoch  13 | [  400/ 1337] | lr 5.00 | ms/batch 220.31 | loss  3.18 | val loss  3.81 | ppl    23.98\n",
      "Pred Sent:   have nfl name allegedly historic m lit penalties deeply way one objects beginning lit relatives ebay penalties breach depression relatives first author depression romance juan depression work work depression relativestic research depression romancegra depression depression novak howard novak they foreign raw\n",
      "| epoch  13 | [  600/ 1337] | lr 5.00 | ms/batch 220.27 | loss  3.23 | val loss  3.91 | ppl    25.34\n",
      "Pred Sent:   a cautious cautious lit scar scar lit penalties 2012 objects a objects objects objects objects novak cautious novak objectsgra a bloom a guards fernandez spur objects objects objects objects to tomorrow reachedwan lived objects depression objects objects objects a lovedfront\n",
      "| epoch  13 | [  800/ 1337] | lr 5.00 | ms/batch 220.38 | loss  3.14 | val loss  3.86 | ppl    23.10\n",
      "Pred Sent:   penaltiesbillion penaltiesoviczan dish scar penalties totallybillion same objects objectschel deemed novak deemedbillion items users world retrieve penaltiesleg penalties deemed deciding deemed include pond back vol rawouriebillionactivatebillionrierie backlan vol\n",
      "| epoch  13 | [ 1000/ 1337] | lr 5.00 | ms/batch 220.72 | loss  3.16 | val loss  3.85 | ppl    23.46\n",
      "Pred Sent:   has cautiousrich lit properties asking lit para lit 3,000 same 23 however pipeline objects include korean jones slur users</s> six reached youngest six glenn keep glenn glenn lit</s>-1 allen released israel glenn include novak users users</s> ray glenn\n",
      "| epoch  13 | [ 1200/ 1337] | lr 5.00 | ms/batch 220.86 | loss  3.17 | val loss  3.84 | ppl    23.87\n",
      "Pred Sent:   has defectiest lit point hugo lit penaltiesique response hand chad novak representgrand novak novak novak beef novak its placing placing lit platformabsolutely novak beef novak lits lit retrieve representiest representatives include novak self scars flaw demonstration\n",
      "| epoch  14 | [  200/ 1337] | lr 5.00 | ms/batch 223.68 | loss  3.43 | val loss  3.79 | ppl    30.81\n",
      "Pred Sent:   mean litbi lit askingique keep keepique novak keep lit carriage lit lit novak novak debt slur lit keepshaw investigation lit keep missing carriage carriage depression carriage world lit depression depression institute carriage depression depression keep novakd classes users\n",
      "| epoch  14 | [  400/ 1337] | lr 5.00 | ms/batch 220.42 | loss  3.15 | val loss  3.87 | ppl    23.23\n",
      "Pred Sent:  ed bloom bloom valentine point nose responsible penalties citizens novakd objects bad foreign citizens novak novak breach encounter jobs so watched depression envoy novak diners deciding kru encounter surveyd objects novak novak novak buy encounter novakwing novak an loved raw\n",
      "| epoch  14 | [  600/ 1337] | lr 5.00 | ms/batch 220.71 | loss  3.19 | val loss  3.93 | ppl    24.18\n",
      "Pred Sent:   to scariest lit) children lit short users objects u objects objects lit objects novak scoring short objects users a wheatringbound novak short placed nor mechanism ibrahimovic a rum treating novakju spoken iii novak confronted novak a chad dawn\n",
      "| epoch  14 | [  800/ 1337] | lr 5.00 | ms/batch 220.23 | loss  3.09 | val loss  3.93 | ppl    22.06\n",
      "Pred Sent:  65 scar removing bull eightactivate 3,000 penalties totally 3,000 them objects65 creating cautious novak65 novak itemswan in needed 3,000pet penalties tampa 3,000 3,000 objects carriage<s> dragon writeou victim workedactivate novak ago novak inlan vol\n",
      "| epoch  14 | [ 1000/ 1337] | lr 5.00 | ms/batch 219.92 | loss  3.13 | val loss  3.94 | ppl    22.88\n",
      "Pred Sent:   hasiestrich lit friendly pen lit paramother 3,000 head alleging however surface objects novak expressbillion slur users them cover shar owners platform platform placed carriagehouse turned them dragon however releasedovic glenn sweetbillionrich scar them loved however\n",
      "| epoch  14 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.90 | loss  3.13 | val loss  3.88 | ppl    22.86\n",
      "Pred Sent:   has defectiest litfriend rig litfra collect novak her warned hague afterwards emma novakwith debt demonstration novak would defect include lit platformabsolutely placed include novak 1.5 will write pra writeiest was seeking novak include novak has name retired\n",
      "| epoch  15 | [  200/ 1337] | lr 5.00 | ms/batch 223.07 | loss  3.40 | val loss  3.80 | ppl    29.97\n",
      "Pred Sent:   devastated depression regarding lit rawique lit depressionique depression has 08:0 patientsgeneral citizens citizens patients debt slur users patients 08:0 alerted morris powell missing dish housing citizens lits author allen morris institute 2012 seeking novak citizens novakd tip novak\n",
      "| epoch  15 | [  400/ 1337] | lr 5.00 | ms/batch 219.92 | loss  3.08 | val loss  3.87 | ppl    21.84\n",
      "Pred Sent:   users lit seageneral asking asking name mique novak<s> ceremony hull running m novak novak breach depression jobs have asking depression idea associated m m kru generate president president overwhelmed county novak novak m encounter novak chemical novak an novak novak\n",
      "| epoch  15 | [  600/ 1337] | lr 5.00 | ms/batch 219.89 | loss  3.15 | val loss  3.99 | ppl    23.42\n",
      "Pred Sent:   scar scariestfriend scar scar spur penalties universal objects would objects objects youngest objects novak cautious novak objects objects have wheat wheatbound shar spur deciding systems objects upon a awarded scar novak novak spoken scar novak scar scar a scar affordable\n",
      "| epoch  15 | [  800/ 1337] | lr 5.00 | ms/batch 220.67 | loss  3.04 | val loss  4.06 | ppl    20.87\n",
      "Pred Sent:   penaltiesrating penaltiesovic objects rou scar penalties isis way sameggy carriagegeneralship include soul debt items kris beingjack depression seeking qpr blasio buckingham include include pond u chad representatives include qpr president include include include include being include vol\n",
      "| epoch  15 | [ 1000/ 1337] | lr 5.00 | ms/batch 220.74 | loss  3.07 | val loss  3.94 | ppl    21.48\n",
      "Pred Sent:   hasiest considered denies bloom relations bathique separated 3,000 – tip novak bath objects novak novak jones slur novak</s> depression depression bath novak peanut placed novak novak shootout</s> limited novak novak worcester cautious worcester novak cautious include</s> relations bath\n",
      "| epoch  15 | [ 1200/ 1337] | lr 5.00 | ms/batch 220.48 | loss  3.08 | val loss  3.96 | ppl    21.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Sent:  john panamaiest lit afterwards trace lit fianc65 lit them litgrand litgrand novak want novak demonstration lit them placing depression lit 2002 depression juice beef lit lit would lit pra ricaiest want depression novakrich novakd 08:4 started\n",
      "| epoch  16 | [  200/ 1337] | lr 5.00 | ms/batch 224.35 | loss  3.37 | val loss  3.84 | ppl    29.14\n",
      "Pred Sent:   mean scariest keep raw dish keep parapet doubt down author novak rawship novak novak novak remain castle world novakarian maj keepkel placed nor novak novak world sheriff novak novak novak novak novak novak passport novak world keep novak\n",
      "| epoch  16 | [  400/ 1337] | lr 5.00 | ms/batch 220.60 | loss  3.05 | val loss  3.98 | ppl    21.09\n",
      "Pred Sent:  ed lit name cafe lit nose five mosspet novak in reached in lit affidavit novak novak novak beef five world five affordableiyapet five m beefpet rupert president qualifier gates novak novak in seekingpet turned novak inlan in\n",
      "| epoch  16 | [  600/ 1337] | lr 5.00 | ms/batch 220.21 | loss  3.09 | val loss  4.07 | ppl    21.96\n",
      "Pred Sent:   short cautious bloom gar raw somehow swift keeppet awarded would objects activist awarded objects novak cautious wheatpetgrad wheat 2020 morris 2. muhammad buckingham kir objects president president users upon depression properties spoken scarbillion cuddle scar a considered rosberg\n",
      "| epoch  16 | [  800/ 1337] | lr 5.00 | ms/batch 220.65 | loss  2.99 | val loss  4.00 | ppl    19.92\n",
      "Pred Sent:   penalties report removing complaints anti lambert darwiniquepet report sameggy acute rory mud running cautious depression itemswan them go vitaminpet qprdentver dating chemical pond shot disaster teamclaim blackpool fell depressionfinalwan scar, inaugural ethiopia\n",
      "| epoch  16 | [ 1000/ 1337] | lr 5.00 | ms/batch 220.83 | loss  3.01 | val loss  4.06 | ppl    20.23\n",
      "Pred Sent:   has stairsrich keep running announce appealing thompson alerted 3,000 them rack running running objects running thompson eaten beeftion them tomorrow sellingpet keep stairs appealing eaten eaten tomorrow \"\" tomorrow stairs policytion objects 3,000 novakrich include running stairs nasty\n",
      "| epoch  16 | [ 1200/ 1337] | lr 5.00 | ms/batch 220.95 | loss  3.02 | val loss  3.99 | ppl    20.49\n",
      "Pred Sent:  iest defectiest lit scar confirm tracefra retrievemedd exportgrand ticketgrand romance want debt beef users would devastated 2020 lit platform depression dish 1.5 objects lit will users retrieve romanceiest 2009, depression novak users scar users flaw 2020\n",
      "| epoch  17 | [  200/ 1337] | lr 5.00 | ms/batch 229.28 | loss  3.36 | val loss  3.86 | ppl    28.91\n",
      "Pred Sent:   mean neighborhood chad keep asking chad chad quick powell chad were bike obsessionose missing cuttingver direct slur chadd paddle cope 23- keep missing dish nor encounter review them7,000 users depression considered arena depression novak shootout novak havewan passage\n",
      "| epoch  17 | [  400/ 1337] | lr 5.00 | ms/batch 227.17 | loss  2.99 | val loss  3.99 | ppl    19.85\n",
      "Pred Sent:  ed litbatni lit nose lit town deeply caretaker them reached mossovic affidavit novak novak novak experienced experienced world author depression hol novakkel deciding nor name lit president qualifiership rica foreign depression depression scored packaging harvey five foreign town\n",
      "| epoch  17 | [  600/ 1337] | lr 5.00 | ms/batch 220.36 | loss  3.05 | val loss  4.16 | ppl    21.06\n",
      "Pred Sent:   a scarflight lit joked 2012 2012 keep bulgersocial a objects objectsovic objects novak cautious ii objects cafe have wheat wheatbound keep solution deciding objects objects a a usersship tear properties spoken depression novak cuddle objects a a a\n",
      "| epoch  17 | [  800/ 1337] | lr 5.00 | ms/batch 219.57 | loss  2.95 | val loss  4.07 | ppl    19.02\n",
      "Pred Sent:   penalties mouth removing complaints anti dish para penalties occur objects back scarclaim franchise running running rust historic itemswan backjack shotpet penalties spurver nor include pond back awarded surrounding awarded foreign mouthactivatebillion inaugural anti back beef vol\n",
      "| epoch  17 | [ 1000/ 1337] | lr 5.00 | ms/batch 219.65 | loss  2.96 | val loss  4.08 | ppl    19.35\n",
      "Pred Sent:   has scar ryder poster started much scar penalties scarver three disappointed rh\"\" objects depression cautiousver pornver them depression bo broadcaster bo porn placed scar scar scar \"\" unknown fell releasedwan algeria seeking cautious ca include access pakistan institute\n",
      "| epoch  17 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.87 | loss  2.98 | val loss  4.06 | ppl    19.73\n",
      "Pred Sent:  home defectiestiest point rig engage para novak caption will export novak executive export novak want novak rookie novak \"\" qpr suspicionben qprabsolutely adopted beef novak novak would write caption novakiest want caption novak caption novaks appealing write\n",
      "| epoch  18 | [  200/ 1337] | lr 5.00 | ms/batch 222.60 | loss  3.32 | val loss  3.91 | ppl    27.63\n",
      "Pred Sent:   devastated scarpower keep raw dish keep canadaique sovereign same objects okayose running novak65 debt slur users keepshaw alerted scar keep complaints dish norpet carriage 2012 users users novak insisted 2012 users novak users users users users users\n",
      "| epoch  18 | [  400/ 1337] | lr 5.00 | ms/batch 219.59 | loss  2.94 | val loss  4.04 | ppl    18.83\n",
      "Pred Sent:  ed chemical name deter lit 2012 five penaltiesique five them reached bad franchise foreign authentic zero breach dy through world restore depression foreignpet depression placed u depression rupert 2012 qualifier foreignou foreign 2012 depression scored chemical scar users foreignou\n",
      "| epoch  18 | [  600/ 1337] | lr 5.00 | ms/batch 219.57 | loss  3.00 | val loss  4.11 | ppl    20.17\n",
      "Pred Sent:   a scar penaltiesfriend objects ibrahimovic swiftdavidpet cautious a objects activistgrand objects author cautious ii objects to a 2012 cautious objects dollars muhammad spoken cautious objects to to a tony include episode spoken include include include to a to loved\n",
      "| epoch  18 | [  800/ 1337] | lr 5.00 | ms/batch 219.26 | loss  2.91 | val loss  4.07 | ppl    18.43\n",
      "Pred Sent:   penalties hip removingovic objects rou want penaltiesique 3,000,lee acuteovic steer novak billy pond itemswan being watched weighed tar penalties male placed glenn include steer, trade citizens switch foreign buyactivate novakwan novak,lan vol\n",
      "| epoch  18 | [ 1000/ 1337] | lr 5.00 | ms/batch 219.40 | loss  2.91 | val loss  4.13 | ppl    18.36\n",
      "Pred Sent:   has scarrich lit author novak novakmed alerted 3,000 \"\" warned partner similar objects novak cautious penalties slurdd ceodpet novak alerted placed novakhouse litd-1 author novak hiding objects alerted novakrich usersd other novak\n",
      "| epoch  18 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.59 | loss  2.95 | val loss  4.05 | ppl    19.03\n",
      "Pred Sent:  ique defectgrand mud lit sector lit penaltiesique chad will chad column ticket running novak cautious novak rookieng retrieve devastated depression lit platform depression placed cuddle objects lits chad retrieve novakiest archaeologist depression novakhen novaks flaw brawl\n",
      "| epoch  19 | [  200/ 1337] | lr 5.00 | ms/batch 222.58 | loss  3.29 | val loss  3.92 | ppl    26.72\n",
      "Pred Sent:   has terrorists65 deliberately asking asking chad ibrahimovicpet has has mail fernando shares churchill £1565 depressionflightwandshaw australiaflight reveals name placedflight name snapped has name name depression sub 2012 depression depression 2012 purchased -ou ca\n",
      "| epoch  19 | [  400/ 1337] | lr 5.00 | ms/batch 219.63 | loss  2.89 | val loss  4.13 | ppl    17.94\n",
      "Pred Sent:   vol allowsbi deter lit asking responsible m deeply aid being demolish watson lit affidavit novak65 targeted encounter asking world ebay thrill palestinian function blunder deciding rupert god rupert an romance provided novak point buy depression scored chemical asking porn loved novak\n",
      "| epoch  19 | [  600/ 1337] | lr 5.00 | ms/batch 219.57 | loss  2.97 | val loss  4.40 | ppl    19.55\n",
      "Pred Sent:  <s> lit flaw foreign lit asking lit -petsocial to objects breach to objects author cautious orleansworld cautious<s> depression cautious morris keep depression deciding complaints objects appealing<s> penalties novak novak episode spoken depression novak cautious scar<unk> to involve\n",
      "| epoch  19 | [  800/ 1337] | lr 5.00 | ms/batch 219.88 | loss  2.87 | val loss  4.09 | ppl    17.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Sent:   penalties scar removing sto objects dish scar penaltiespet 3,000 will objects include franchise volunteers novak billy jones itemswan<s> santiago rupertpet novak approximately deciding rupert include lit, 6-2 novak novak franchise mainly constant novak ca persistent<unk>iest 999\n",
      "| epoch  19 | [ 1000/ 1337] | lr 5.00 | ms/batch 219.70 | loss  2.87 | val loss  4.18 | ppl    17.72\n",
      "Pred Sent:   her scar decided charm toyota however valentine properties portion nonprofit being jeff however aim however etihad soul80crib individuals being depression aim however however however buckingham norhousecrib their qualifier howeverwan hiding pen buckingham buckingham packaging alexandra. other however\n",
      "| epoch  19 | [ 1200/ 1337] | lr 5.00 | ms/batch 219.96 | loss  2.92 | val loss  4.08 | ppl    18.63\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "log_interval = 200\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for i, (x, n, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        n = n.to(device)\n",
    "        y = y.permute(1,0).to(device)\n",
    "      \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, y)\n",
    "        loss = criterion(output.view(MAX_WORD_N, n_vocab, -1), y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            test_loss = evaluate(model, test_loader)\n",
    "            print('| epoch {:3d} | [{:5d}/{:5d}] | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | val loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, len(train_loader),scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, test_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
