{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "SHUFFEL_SIZE = 1024\n",
    "\n",
    "learning_rate = 3e-5\n",
    "\n",
    "model_size = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_size)\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_size)\n",
    "\n",
    "task_specific_params = model.config.task_specific_params\n",
    "if task_specific_params is not None:\n",
    "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_size)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "en_de_prefix = tf.reshape(tokenizer.encode(\"summarize: en_to_ger \", return_tensors=\"tf\"), (-1,))\n",
    "de_en_prefix = tf.reshape(tokenizer.encode(\"summarize: ger_to_en \", return_tensors=\"tf\"), (-1,))\n",
    "en_en_prefix = tf.reshape(tokenizer.encode(\"summarize: en_to_en \", return_tensors=\"tf\"), (-1,))\n",
    "de_de_prefix = tf.reshape(tokenizer.encode(\"summarize: ger_to_ger \", return_tensors=\"tf\"), (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 19, 3, 9, 16513, 5]\n",
      "[48, 19, 3, 9, 16513, 5, 1]\n",
      "this is a sentences.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"this is a sentences.\"))\n",
    "print(tokenizer.encode(\"this is a sentences.</s>\"))\n",
    "print(tokenizer.decode([48, 19, 3, 9, 16513, 5, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfrom(x):\n",
    "    x = \" \".join(x.split(\"; \")[1:])\n",
    "    x = re.sub(\"'(.*)'\", r\"\\1\", x)\n",
    "    return x + \"</s>\"\n",
    "\n",
    "def transfrom_en(x):\n",
    "    x = re.sub(\"'(.*)'\", r\"\\1\", x)\n",
    "    return x + \"</s>\"\n",
    "\n",
    "def tokenize_articles(text):\n",
    "    ids = tokenizer.encode_plus(text, return_tensors=\"tf\", max_length=(512-8), pad_to_max_length=True) \n",
    "    return tf.squeeze(ids['input_ids']), tf.squeeze(ids['attention_mask'])\n",
    "        \n",
    "def tokenize_highlights(text):\n",
    "    y = tokenizer.encode(text, return_tensors=\"tf\", max_length=150, pad_to_max_length=True)\n",
    "    y = tf.squeeze(y)\n",
    "    y_ids = y[:-1]\n",
    "    lm_labels = tf.identity(y[1:])\n",
    "    lm_labels = tf.where(tf.equal(y[1:],pad_token_id), -100, lm_labels)  \n",
    "\n",
    "    return y, y_ids, lm_labels\n",
    "\n",
    "\n",
    "def get_german_data(name):\n",
    "    article_path = \"../data/%s/articles_german\" % name\n",
    "    highlights_path = \"../data/%s/highlights_german\" % name\n",
    "\n",
    "    articles = [transfrom(x.rstrip()) for x in open(article_path).readlines()]\n",
    "    highlights = [transfrom(x.rstrip()) for x in open(highlights_path).readlines()]\n",
    "    return articles, highlights\n",
    "  \n",
    "def get_english_data(name):\n",
    "    article_path = \"../data/%s/article\" % name\n",
    "    highlights_path = \"../data/%s/highlights\" % name\n",
    "\n",
    "    articles = [transfrom_en(x.rstrip()) for x in open(article_path).readlines()]\n",
    "    highlights = [transfrom_en(x.rstrip()) for x in open(highlights_path).readlines()]\n",
    "    return articles, highlights\n",
    "    \n",
    "def get_tokinized_ds(articles, highlights):\n",
    "    x = [] \n",
    "    x_mask = []\n",
    "    for x_i in articles:\n",
    "        t1, t2 = tokenize_articles(x_i)\n",
    "        x.append(t1)\n",
    "        x_mask.append(t2)\n",
    "        \n",
    "    y = []\n",
    "    y_ids = [] \n",
    "    y_labels = []\n",
    "    for y_i in highlights:\n",
    "        t1, t2, t3 = tokenize_highlights(y_i)\n",
    "        y.append(t1)\n",
    "        y_ids.append(t2)\n",
    "        y_labels.append(t3)\n",
    "        \n",
    "        \n",
    "    return x, x_mask, y, y_ids, y_labels\n",
    "\n",
    "def get_translated_ds(name):\n",
    "    ger_articles, ger_highlights = get_german_data(name)\n",
    "    en_articles, en_highlights = get_english_data(name)\n",
    "    \n",
    "    return get_tokinized_ds(ger_articles, ger_highlights), get_tokinized_ds(en_articles, en_highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_translated_ds(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = get_translated_ds(\"val\")\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corss_lingual_test_cnn_daily_mail.\n",
      "Saved corss_lingual_val_cnn_daily_mail.\n"
     ]
    }
   ],
   "source": [
    "skip = False\n",
    "drive_path = \"../data\"\n",
    "\n",
    "# Prepare tf.Examples and tf.Features and write them as TFRecords\n",
    "def save_tfrecord_to_bucket(features_dataset, gdrive_folder, file_name):\n",
    "    with tf.compat.v1.python_io.TFRecordWriter(f\"{gdrive_folder}/{file_name}.tfrecord\") as tfwriter:\n",
    "        for train_feature in features_dataset:\n",
    "            (ger_x, ger_x_mask, ger_y, ger_y_ids, ger_y_labels), (en_x, en_x_mask, en_y, en_y_ids, en_y_labels) = train_feature\n",
    "            feature_key_value_pair = {\n",
    "                'ger_x': tf.train.Feature(int64_list=tf.train.Int64List(value=ger_x)),\n",
    "                'ger_x_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=ger_x_mask)),\n",
    "                'ger_y': tf.train.Feature(int64_list=tf.train.Int64List(value=ger_y)),\n",
    "                'ger_y_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=ger_y_ids)),\n",
    "                'ger_y_labels': tf.train.Feature(int64_list=tf.train.Int64List(value=ger_y_labels)),\n",
    "                'en_x': tf.train.Feature(int64_list=tf.train.Int64List(value=en_x)),\n",
    "                'en_x_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=en_x_mask)),\n",
    "                'en_y': tf.train.Feature(int64_list=tf.train.Int64List(value=en_y)),\n",
    "                'en_y_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=en_y_ids)),\n",
    "                'en_y_labels': tf.train.Feature(int64_list=tf.train.Int64List(value=en_y_labels))\n",
    "            }\n",
    "            features = tf.train.Features(feature=feature_key_value_pair)\n",
    "            example = tf.train.Example(features=features)\n",
    "\n",
    "            tfwriter.write(example.SerializeToString())\n",
    "    print(f\"Saved {file_name}.\")\n",
    "\n",
    "save_tfrecord_to_bucket(test_ds, drive_path, \"corss_lingual_test_cnn_daily_mail\")\n",
    "save_tfrecord_to_bucket(val_ds, drive_path, \"corss_lingual_val_cnn_daily_mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corss_lingual_train_cnn_daily_mail.\n"
     ]
    }
   ],
   "source": [
    "train = get_translated_ds(\"train\")\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train) \n",
    "save_tfrecord_to_bucket(train_ds, drive_path, \"corss_lingual_train_cnn_daily_mail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "MAX_ARTICLE_LEN = 512\n",
    "MAX_HIGHLIGHT_LEN = 150\n",
    "bucket = \"\"\n",
    "GLOBAL_BATCH_SIZE = 8\n",
    "\n",
    "def get_tfrecord_dataset(drive_path, file_name):\n",
    "    features = {\n",
    "        'ger_x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
    "        'ger_x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
    "        'ger_y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "        'ger_y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
    "        'ger_y_labels': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
    "\n",
    "        'en_x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
    "        'en_x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
    "        'en_y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
    "        'en_y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
    "        'en_y_labels': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
    "    }\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(f\"../data/{file_name}.tfrecord\")\n",
    "\n",
    "    # Taken from the TensorFlow models repository: https://github.com/tensorflow/models/blob/befbe0f9fe02d6bc1efb1c462689d069dae23af1/official/nlp/bert/input_pipeline.py#L24\n",
    "    def decode_record(record, features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.io.parse_single_example(record, features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "\n",
    "    def select_data_from_record(record):\n",
    "        i  = np.random.randint(4) \n",
    "        if i == 0:\n",
    "            return tf.concat([de_de_prefix, record['ger_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['ger_x_mask']], axis=0), record['ger_y'], record['ger_y_ids'], record['ger_y_labels']\n",
    "        elif i == 1:\n",
    "            return tf.concat([en_de_prefix, record['en_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['en_x_mask']], axis=0), record['ger_y'], record['ger_y_ids'], record['ger_y_labels']\n",
    "        elif i == 2:\n",
    "            return tf.concat([de_en_prefix, record['ger_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['ger_x_mask']], axis=0), record['en_y'], record['en_y_ids'], record['en_y_labels']\n",
    "        elif i == 3:\n",
    "            return tf.concat([en_en_prefix, record['en_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['en_x_mask']], axis=0), record['en_y'], record['en_y_ids'], record['en_y_labels']\n",
    " \n",
    "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
    "    dataset = dataset.map(select_data_from_record)\n",
    "    dataset = dataset.shuffle(100)\n",
    "    return dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "train_dataset = get_tfrecord_dataset(bucket, \"corss_lingual_train_cnn_daily_mail\")\n",
    "train_dataset.prefetch(1024)\n",
    "\n",
    "validation_dataset = get_tfrecord_dataset(bucket, \"corss_lingual_val_cnn_daily_mail\")\n",
    "test_dataset = get_tfrecord_dataset(bucket, \"corss_lingual_test_cnn_daily_mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[21603,    10,     3, ...,   324,  3348, 14519],\n",
      "       [21603,    10,     3, ...,    35,    74,     3],\n",
      "       [21603,    10,     3, ...,    18,  2703,   208],\n",
      "       ...,\n",
      "       [21603,    10,     3, ...,  7453,     6,     3],\n",
      "       [21603,    10,     3, ...,  6394,    93, 12483],\n",
      "       [21603,    10,     3, ...,     3, 18444, 22186]], dtype=int32)>, <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, <tf.Tensor: shape=(8, 150), dtype=int32, numpy=\n",
      "array([[19445,  1064,  2165, ...,     0,     0,     0],\n",
      "       [ 2318,  3304,  1699, ...,    16,  7457,    11],\n",
      "       [  736,  1273,  2074, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 8100,    52,  7923, ...,     0,     0,     0],\n",
      "       [27292, 18707,    23, ...,     0,     0,     0],\n",
      "       [ 1955,  5531, 16517, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(8, 149), dtype=int32, numpy=\n",
      "array([[19445,  1064,  2165, ...,     0,     0,     0],\n",
      "       [ 2318,  3304,  1699, ...,  1015,    16,  7457],\n",
      "       [  736,  1273,  2074, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 8100,    52,  7923, ...,     0,     0,     0],\n",
      "       [27292, 18707,    23, ...,     0,     0,     0],\n",
      "       [ 1955,  5531, 16517, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(8, 149), dtype=int32, numpy=\n",
      "array([[ 1064,  2165,  2298, ...,  -100,  -100,  -100],\n",
      "       [ 3304,  1699,  9203, ...,    16,  7457,    11],\n",
      "       [ 1273,  2074,    16, ...,  -100,  -100,  -100],\n",
      "       ...,\n",
      "       [   52,  7923,    18, ...,  -100,  -100,  -100],\n",
      "       [18707,    23,    52, ...,  -100,  -100,  -100],\n",
      "       [ 5531, 16517,     6, ...,  -100,  -100,  -100]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for d in train_dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummary",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
