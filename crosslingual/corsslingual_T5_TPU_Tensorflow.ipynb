{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "corsslingual_T5_TPU Tensorflow ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "131d30f7ce9d4ca4950b840d8832477d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea36571e2dc14fc987cba6ffacb0cdbe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6902714d12254272ad2f50ff2ac643b7",
              "IPY_MODEL_a8c9876f3a5547269089240fe75fdd5e"
            ]
          }
        },
        "ea36571e2dc14fc987cba6ffacb0cdbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6902714d12254272ad2f50ff2ac643b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ee15ed80a7dd401da09fee735e8e2cee",
            "_dom_classes": [],
            "description": "Training: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25326dab2ef04019b62cb2ddfe8ef5e5"
          }
        },
        "a8c9876f3a5547269089240fe75fdd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c3fa8208e8744f885a8bcec7cdf230f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1111/? [09:16&lt;00:00,  2.44it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_562544ebf5274e289f195bb55868cd4c"
          }
        },
        "ee15ed80a7dd401da09fee735e8e2cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25326dab2ef04019b62cb2ddfe8ef5e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c3fa8208e8744f885a8bcec7cdf230f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "562544ebf5274e289f195bb55868cd4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGqVkG2-7qfu",
        "colab_type": "text"
      },
      "source": [
        "# T5 TPU Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZIeAC0_tGXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "d72099c8-7890-4a4a-8a7c-3adac59ac1fd"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFkyo148RDH_",
        "colab_type": "text"
      },
      "source": [
        "### Imports\n",
        "\n",
        "We'll only be importing the components that we'll use during this tutorial: the TensorFlow model alongside the model specific tokenizer. The last two imports will manage the pre-processing of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhpauvOIJzzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "80277dae-b42a-483a-e82e-c9b9fb6bdcf8"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os\n",
        "from transformers import ( \n",
        "    T5Tokenizer, \n",
        "    TFT5Model, \n",
        "    TFT5ForConditionalGeneration\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB9CLi1ovDxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "SHUFFEL_SIZE = 1024\n",
        "\n",
        "learning_rate = 3e-5\n",
        "\n",
        "model_size = \"t5-base\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jShcjelNEFq0",
        "colab_type": "text"
      },
      "source": [
        "# Building the training system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX4SPt3mHf0L",
        "colab_type": "text"
      },
      "source": [
        "## Strategy\n",
        "\n",
        "We make use of TensorFlow's strategies, which handle the data distribution as well as the distributed training that happens on the devices available. In this example we'll be using a `MirroredStrategy` which can be used to train on a multiple GPUs in a distributed manner. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Bnz94tuBFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "06fab094-b241-4ffb-b39d-68516284e8a0"
      },
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.25.24.114:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.25.24.114:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x7f5901a68400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mIc_Ue7HWBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "701fa924-d167-48f8-db62-5c354086a38e"
      },
      "source": [
        "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of accelerators:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t6ihoFbKoOB",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Dataset with the strategy\n",
        "\n",
        "Here we define a batch size for each replica. We set it to be a multiple of 8 to best leverage the systolic array as defined in the [Google TPU performance guide](https://cloud.google.com/tpu/docs/performance-guide#rule_of_thumb_pick_efficient_values_for_batch_and_feature_dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwCFAt9_iCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE_PER_REPLICA = 6\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "EPOCHS = 5\n",
        "\n",
        "MAX_ARTICLE_LEN = 512\n",
        "MAX_HIGHLIGHT_LEN = 150"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d4j3dIqUbXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "716132fb-269d-4577-e94a-ed74963ffa00"
      },
      "source": [
        "GLOBAL_BATCH_SIZE"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TBaFqenUXq8",
        "colab_type": "text"
      },
      "source": [
        "### Retrieving the TFRecord dataset\n",
        "\n",
        "The TFRecord dataset is now entirely processed and ready to be used as input by our training loop. We load it, shuffle it and batch it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qElWBW3rGrkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(model_size)\n",
        "en_de_prefix = tf.reshape(tokenizer.encode(\"summarize: en_to_ger \", return_tensors=\"tf\"), (-1,))\n",
        "de_en_prefix = tf.reshape(tokenizer.encode(\"summarize: ger_to_en \", return_tensors=\"tf\"), (-1,))\n",
        "en_en_prefix = tf.reshape(tokenizer.encode(\"summarize: en_to_en \", return_tensors=\"tf\"), (-1,))\n",
        "de_de_prefix = tf.reshape(tokenizer.encode(\"summarize: ger_to_ger \", return_tensors=\"tf\"), (-1,))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmMlC1i0COBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "bucket = \"gs://tpu-bucket-cnn-daily-mail\"\n",
        "\n",
        "def get_tfrecord_dataset(drive_path, file_name):\n",
        "    features = {\n",
        "        'ger_x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
        "        'ger_x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
        "        'ger_y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
        "        'ger_y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
        "        'ger_y_labels': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
        "\n",
        "        'en_x': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
        "        'en_x_mask': tf.io.FixedLenFeature([MAX_ARTICLE_LEN-8], tf.int64),\n",
        "        'en_y': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN], tf.int64),\n",
        "        'en_y_ids': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
        "        'en_y_labels': tf.io.FixedLenFeature([MAX_HIGHLIGHT_LEN - 1], tf.int64),\n",
        "    }\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(f\"{drive_path}/{file_name}.tfrecord\")\n",
        "\n",
        "    # Taken from the TensorFlow models repository: https://github.com/tensorflow/models/blob/befbe0f9fe02d6bc1efb1c462689d069dae23af1/official/nlp/bert/input_pipeline.py#L24\n",
        "    def decode_record(record, features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.io.parse_single_example(record, features)\n",
        "\n",
        "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "        # So cast all int64 to int32.\n",
        "        for name in list(example.keys()):\n",
        "            t = example[name]\n",
        "            if t.dtype == tf.int64:\n",
        "                t = tf.cast(t, tf.int32)\n",
        "            example[name] = t\n",
        "        return example\n",
        "\n",
        "\n",
        "    def select_data_from_record(record):\n",
        "        i  = np.random.randint(4) \n",
        "        if i == 0:\n",
        "            return tf.concat([de_de_prefix, record['ger_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['ger_x_mask']], axis=0), record['ger_y'], record['ger_y_ids'], record['ger_y_labels']\n",
        "        elif i == 1:\n",
        "            return tf.concat([en_de_prefix, record['en_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['en_x_mask']], axis=0), record['ger_y'], record['ger_y_ids'], record['ger_y_labels']\n",
        "        elif i == 2:\n",
        "            return tf.concat([de_en_prefix, record['ger_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['ger_x_mask']], axis=0), record['en_y'], record['en_y_ids'], record['en_y_labels']\n",
        "        elif i == 3:\n",
        "            return tf.concat([en_en_prefix, record['en_x']], axis=0), tf.concat([tf.ones(8, dtype=tf.int32), record['en_x_mask']], axis=0), record['en_y'], record['en_y_ids'], record['en_y_labels']\n",
        " \n",
        "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
        "    dataset = dataset.map(select_data_from_record)\n",
        "    dataset = dataset.shuffle(100)\n",
        "    return dataset.batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "train_dataset = get_tfrecord_dataset(bucket, \"corss_lingual_train_cnn_daily_mail\")\n",
        "train_dataset.prefetch(1024)\n",
        "\n",
        "validation_dataset = get_tfrecord_dataset(bucket, \"corss_lingual_val_cnn_daily_mail\")\n",
        "test_dataset = get_tfrecord_dataset(bucket, \"corss_lingual_test_cnn_daily_mail\")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P6RtkHKaElt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkXR1MK0J7Zp",
        "colab_type": "text"
      },
      "source": [
        "There is an additional step here to distribute the dataset among the different TPU cores. We make use of a strategy method to do so.\n",
        "\n",
        "Every item held in the dataset (which is a batched dataset) will now be split over the TPU workers. As the TPU we're using has 8 workers and our batch is of size 64, every example will be evenly split in batches of (64 / 8 =) 8 and distributed across workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv2zz8vYJ7vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "validation_dist_dataset = strategy.experimental_distribute_dataset(validation_dataset)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmC9J6brHN0B",
        "colab_type": "text"
      },
      "source": [
        "## Model creation\n",
        "\n",
        "We create a function that will instantiate a new model when called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1C-9yTS2GTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn():\n",
        "    return TFT5ForConditionalGeneration.from_pretrained(model_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS1dc5mvHXNa",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters initialization\n",
        "\n",
        "While in the strategy's scope, we define a sparse categorical crossentropy loss. We define a method `compute_loss` which will be called to compute the loss between the model's prediction and the expected result (or label).\n",
        "\n",
        "In order to measure the accuracy during training and evaluation, we define two metrics which are both sparse categorical accuracy.\n",
        "\n",
        "Finally, we initialize a model and create an optimizer object using the Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rItpKUam2JSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "33b62fd4-141f-4582-c5c7-6fbe36892ec2"
      },
      "source": [
        "with strategy.scope():\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\n",
        "\n",
        "    def compute_loss(labels, predictions):\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "    test_loss_metric = tf.keras.metrics.Mean(name='test_loss')\n",
        "    test_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "    train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "    train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy')\n",
        "    \n",
        "    model = model_fn()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.modeling_tf_utils:All model checkpoint weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "WARNING:transformers.modeling_tf_utils:All the weights of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIzuZKDMIuft",
        "colab_type": "text"
      },
      "source": [
        "## Steps\n",
        "\n",
        "We create two functions that will be called during the training and test steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZIqCzhL2R6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    def train_step(inputs):\n",
        "        input_ids, input_mask, y, y_ids, lm_labels = inputs\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(input_ids, attention_mask=input_mask, decoder_input_ids=y_ids, lm_labels=lm_labels, training=True)[0]  # Gather only the outputs of the text-classification head\n",
        "            loss = compute_loss(y[:, 1:], predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        train_loss_metric.update_state(loss)\n",
        "        train_accuracy_metric.update_state(y[:, 1:], predictions)\n",
        "\n",
        "    def test_step(inputs):\n",
        "        input_ids, input_mask, y, y_ids, lm_labels = inputs\n",
        "\n",
        "        predictions = model(input_ids, attention_mask=input_mask, decoder_input_ids=y_ids, lm_labels=lm_labels, training=False)[0]  # Gather only the outputs of the text-classification head\n",
        "        t_loss = compute_loss(y[:, 1:], predictions)\n",
        "\n",
        "        test_loss_metric.update_state(t_loss)\n",
        "        test_accuracy_metric.update_state(y[:, 1:], predictions)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqhnTfbEJ9fa",
        "colab_type": "text"
      },
      "source": [
        "## Training & Evaluation\n",
        "\n",
        "Finally, using all the previously defined attributes, we create two traced tf.function which will execute the training and test steps in a distributed manner. There is no need for them to return anything as the metrics will directly be updated in the steps described beforehand.\n",
        "\n",
        "We loop over the number of epochs, training the model and evaluating it at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_chN1Sy3IXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218,
          "referenced_widgets": [
            "131d30f7ce9d4ca4950b840d8832477d",
            "ea36571e2dc14fc987cba6ffacb0cdbe",
            "6902714d12254272ad2f50ff2ac643b7",
            "a8c9876f3a5547269089240fe75fdd5e",
            "ee15ed80a7dd401da09fee735e8e2cee",
            "25326dab2ef04019b62cb2ddfe8ef5e5",
            "2c3fa8208e8744f885a8bcec7cdf230f",
            "562544ebf5274e289f195bb55868cd4c"
          ]
        },
        "outputId": "1433cd81-2e6a-48cc-f957-2a9b9c2a2dd4"
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "with strategy.scope():\n",
        "    @tf.function\n",
        "    def distributed_train_step(dataset):\n",
        "        strategy.run(train_step, args=(dataset,))\n",
        " \n",
        "\n",
        "    @tf.function\n",
        "    def distributed_test_step(dataset):\n",
        "        strategy.run(test_step, args=(dataset,))\n",
        "\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0.0\n",
        "        training_steps = 10\n",
        "        epoch_step = 0\n",
        "        print_every = 1000\n",
        "\n",
        "        ### Training loop ###\n",
        "        for tensor in tqdm(train_dist_dataset, desc=\"Training\"):\n",
        "            distributed_train_step(tensor)  \n",
        "\n",
        "            train_loss = train_loss_metric.result().numpy().astype(float)\n",
        "            train_accuracy = train_accuracy_metric.result().numpy()\n",
        "\n",
        "            global_step += 1\n",
        "            epoch_step += 1\n",
        "\n",
        "            if epoch_step % print_every == 0:\n",
        "                print(f\"Training step {epoch_step} Accuracy: {train_accuracy}, Training loss: {train_loss}\")\n",
        "\n",
        "\n",
        "        ### Test loop ###\n",
        "        for tensor in tqdm(validation_dist_dataset, desc=\"Evaluating\"):\n",
        "            distributed_test_step(tensor)\n",
        "            \n",
        "        \n",
        "        ### Output results ###\n",
        "        test_accuracy = test_accuracy_metric.result().numpy()\n",
        "        test_loss = test_loss_metric.result().numpy()\n",
        "        print(f'Epoch: [{epoch}] Validation accuracy = {test_accuracy}')\n",
        "\n",
        "        ### Reset metrics ###\n",
        "        test_loss_metric.reset_states()\n",
        "        train_accuracy_metric.reset_states()\n",
        "        train_loss_metric.reset_states()\n",
        "        test_accuracy_metric.reset_states()\n",
        "        epoch_step = 0\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "131d30f7ce9d4ca4950b840d8832477d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', max=1.0, style=ProgressStyleâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training step 1000 Accuracy: 0.6952540278434753, Training loss: 31.111953735351562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk0H6POH1Z6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt_file = os.path.join(bucket, \"checkpoint.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3qpMpDAihqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(ckpt_file) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap2-FMqd3mN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(ckpt_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}