{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0FMn6DnelFC"
   },
   "source": [
    "# Transformer (NMT) Translation\n",
    "- Based on : https://pytorch.org/hub/pytorch_fairseq_translation/\n",
    "    *Author: Facebook AI (fairseq Team)*\n",
    "\n",
    "## Translate CNN Daily Mail\n",
    "We will translate the English Dataset CNN Daily Mail to German to try German text summarization\n",
    "\n",
    "### Model Description\n",
    "\n",
    "The Transformer, introduced in the paper [Attention Is All You Need][1], is a\n",
    "powerful sequence-to-sequence modeling architecture capable of producing\n",
    "state-of-the-art neural machine translation (NMT) systems.\n",
    "\n",
    "Recently, the fairseq team has explored large-scale semi-supervised training of\n",
    "Transformers using back-translated data, further improving translation quality\n",
    "over the original model. More details can be found in [this blog post][2].\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "We require a few additional Python dependencies for preprocessing:\n",
    "-  pip install fastBPE regex requests sacremoses subword_nmt Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2_52dLfLwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 12 11:03:13 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:08:00.0  On |                  N/A |\r\n",
      "| 40%   45C    P5    27W / 260W |   1446MiB / 11016MiB |     15%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1396      G   /usr/lib/xorg/Xorg                           150MiB |\r\n",
      "|    0      2546      G   /usr/lib/xorg/Xorg                           493MiB |\r\n",
      "|    0      2748      G   /usr/bin/gnome-shell                         232MiB |\r\n",
      "|    0      5522      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   542MiB |\r\n",
      "|    0     10825      G   ...p/pycharm-professional/201/jbr/bin/java     8MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zPOebvRelFH"
   },
   "source": [
    "### English-to-German Translation\n",
    "\n",
    "Semi-supervised training with back-translation is an effective way of improving\n",
    "translation systems. In the paper [Understanding Back-Translation at Scale][4],\n",
    "we back-translate over 200 million German sentences to use as additional\n",
    "training data. An ensemble of five of these models was the winning submission to\n",
    "the [WMT'18 English-German news translation competition][5].\n",
    "\n",
    "We can further improved this approach through [noisy-channel reranking][6]. More\n",
    "details can be found in [this blog post][7]. An ensemble of models trained with\n",
    "this technique was the winning submission to the [WMT'19 English-German news\n",
    "translation competition][8].\n",
    "\n",
    "To translate from English to German using one of the models from the winning submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22podDrRelFM"
   },
   "source": [
    "### References\n",
    "\n",
    "- [Attention Is All You Need][1]\n",
    "- [Scaling Neural Machine Translation][3]\n",
    "- [Understanding Back-Translation at Scale][4]\n",
    "- [Facebook FAIR's WMT19 News Translation Task Submission][6]\n",
    "\n",
    "\n",
    "[1]: https://arxiv.org/abs/1706.03762\n",
    "[2]: https://code.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/\n",
    "[3]: https://arxiv.org/abs/1806.00187\n",
    "[4]: https://arxiv.org/abs/1808.09381\n",
    "[5]: http://www.statmt.org/wmt18/translation-task.html\n",
    "[6]: https://arxiv.org/abs/1907.06616\n",
    "[7]: https://ai.facebook.com/blog/facebook-leads-wmt-translation-competition/\n",
    "[8]: http://www.statmt.org/wmt19/translation-task.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from segtok.segmenter import split_single\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write tfds Dataset to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_run:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    cnn_dailymail = tfds.load(name=\"cnn_dailymail\")\n",
    "\n",
    "    train_tfds = cnn_dailymail['train']\n",
    "    test_tfds = cnn_dailymail['test']\n",
    "    val_tfds = cnn_dailymail['validation']\n",
    "\n",
    "    train_ds_iter = tfds.as_numpy(train_tfds)\n",
    "    val_ds_iter = tfds.as_numpy(val_tfds)\n",
    "    test_ds_iter = tfds.as_numpy(test_tfds)\n",
    "\n",
    "\n",
    "    def write_data(iter_dataset, name, path=\"../data/\"):\n",
    "\n",
    "        articles_file = Path(path + name + \"/article\").open(\"w\")\n",
    "        highlights_file = Path(path + name + \"/highlights\").open(\"w\")\n",
    "\n",
    "        for item in iter_dataset:\n",
    "            articles_file.write(item[\"article\"].decode(\"utf-8\") + \"\\n\")\n",
    "            articles_file.flush()\n",
    "            highlights_file.write(item[\"highlights\"].decode(\"utf-8\").replace(\"\\n\", \" \") + \"\\n\")\n",
    "            highlights_file.flush()\n",
    "\n",
    "    write_data(train_ds_iter, \"train\")\n",
    "    write_data(test_ds_iter, \"test\")\n",
    "    write_data(val_ds_iter, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yannik/.cache/torch/hub/pytorch_fairseq_master\n",
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Load an En-De Transformer model trained on WMT'19 data:\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "# Access the underlying TransformerModel\n",
    "assert isinstance(en2de.models[0], torch.nn.Module)\n",
    "\n",
    "# Translate from En-De\n",
    "de = en2de.translate('PyTorch Hub is a pre-trained model repository designed to facilitate research reproducibility.')\n",
    "assert de == 'PyTorch Hub ist ein vorgefertigtes Modell-Repository, das die Reproduzierbarkeit der Forschung erleichtern soll.'\n",
    "\n",
    "# to gpu\n",
    "en2de = en2de.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upEvVDG1fB4t"
   },
   "outputs": [],
   "source": [
    "def read_files(name):\n",
    "    article_path = \"../data/%s/article\" % name\n",
    "    highlights_path = \"../data/%s/highlights\" % name\n",
    "    \n",
    "    articles = [x.rstrip() for x in open(article_path).readlines()]\n",
    "    highlights = [x.rstrip() for x in open(highlights_path).readlines()]\n",
    "    \n",
    "    assert len(articles) == len(highlights)\n",
    "    return articles, highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_in_sentences(text):\n",
    "    return split_single(text)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, articles):\n",
    "        self.x = articles\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentences = split_in_sentences(self.x[index]) \n",
    "        ret_x = []\n",
    "        for i, sent in enumerate(sentences): \n",
    "            if i == 0 and sent[:2] == \"By\":\n",
    "                ret_x.append(sent)\n",
    "                \n",
    "            else:\n",
    "                ret_x.append(sent)\n",
    "        \n",
    "        return ret_x\n",
    "    \n",
    "    @staticmethod\n",
    "    def transfrom(x):\n",
    "        x = x.lower()\n",
    "        x = re.sub(\"'(.*)'\", r\"\\1\", x)\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"train\"\n",
    "articles, highlights = read_files(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_ds = MyDataset(articles)\n",
    "\n",
    "highlights_ds = MyDataset(highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FileWriter:\n",
    "    def __init__(self, ds_name, name, path=\"../data/\"):\n",
    "        self.path = path + ds_name + \"/\"+ name + \"_german\"\n",
    "        self.file = Path(self.path).open(\"w\")\n",
    "        \n",
    "    def write_translated(self, i, list_str):    \n",
    "        result = str(i) + \"; \"\n",
    "        for item_str in list_str:\n",
    "            result += item_str + \" \"\n",
    "        self.file.write(result.replace(\"\\n\", \" \") + \"\\n\")\n",
    "        self.file.flush()\n",
    "    \n",
    "    def get_last_index(self):\n",
    "        with open(self.path) as fileObj:\n",
    "            ret_list = list(fileObj)\n",
    "            if len(ret_list) > 0: \n",
    "                return ret_list[-1].split(\";\")[0]\n",
    "            else: \n",
    "                return 0\n",
    "        \n",
    "    \n",
    "file_writer = FileWriter(\"train\", \"articles\")  \n",
    "file_writer.get_last_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(ds, ds_name, name, log_interval=1000):\n",
    "    len_ds = len(ds)\n",
    "\n",
    "    file_writer = FileWriter(ds_name, name)\n",
    "    first_index = file_writer.get_last_index()\n",
    "    start_time = time.time()\n",
    "    for i in range(first_index, len_ds):\n",
    "        predictions = []\n",
    "        for x in ds[i]:\n",
    "            pred = en2de.translate(x)\n",
    "            predictions.append(pred)  \n",
    "        file_writer.write_translated(i, predictions)\n",
    "        \n",
    "        elapsed = time.time() - start_time  \n",
    "        if (i + 1 % log_interval) == 0:\n",
    "            print(\"| [{:5d}/{:5d}] | ms/ds_point {:5.2f} |\".format(i, len(articles_ds), (elapsed * 1000 / log_interval)))\n",
    "       \n",
    "        \n",
    "translate(articles_ds, \"train\", \"articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(articles_loader, \"train\", \"highliths\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of pytorch_fairseq_translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "textsummary",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
