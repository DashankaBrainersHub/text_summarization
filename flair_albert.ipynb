{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import BertEmbeddings, DocumentPoolEmbeddings\n",
    "from segtok.segmenter import split_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert = BertEmbeddings(bert_model_or_path=\"albert-base-v2\")\n",
    "\n",
    "albert_embedding = DocumentPoolEmbeddings([albert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "tensor([-0.6863, -0.5820,  1.0685,  ...,  0.7118,  0.6721,  0.5402],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "sent = Sentence(\"Berlin and Munich are nice cities .\")\n",
    "albert_embedding.embed(sent)\n",
    "\n",
    "embedd_result = sent.get_embedding()\n",
    "print(embedd_result.shape)\n",
    "print(embedd_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: cnn_dailymail/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset cnn_dailymail (/home/yannik/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/yannik/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0\n"
     ]
    }
   ],
   "source": [
    "cnn_dailymail = tfds.load(name=\"cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tds = cnn_dailymail['train']\n",
    "test_tds = cnn_dailymail['test']\n",
    "val_tds = cnn_dailymail['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "- for faster training, we will clean the data, compute the Albert-Base Embedding of all articles and save it to Files, so that we don't have to do it while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def map_func(features):\n",
    "    article_text = normalize_text(features[\"article\"])\n",
    "    highlights_text = normalize_text(features['highlights'])\n",
    "    \n",
    "    return article_text.numpy().decode('UTF-8'), highlights_text.numpy().decode('UTF-8')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_of_article(article, i):\n",
    "    list_embedding = []\n",
    "    \n",
    "    sentences = split_single(article)\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        # cuts to long sentences of \n",
    "        if len(sentence) > 750:\n",
    "            sentence = sentence[:750]\n",
    "            \n",
    "        if len(sentence) > 1:\n",
    "            sent = Sentence(sentence)\n",
    "\n",
    "            albert_embedding.embed(sent)\n",
    "            x = sent.get_embedding()\n",
    "            x = x.to('cpu').detach().numpy()\n",
    "            list_embedding.append(x)\n",
    "    return list_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd_ds(ds):\n",
    "    new_ds = []\n",
    "    for i, item in enumerate(ds):\n",
    "        article, higlights = map_func(item)\n",
    "        x = get_embedding_of_article(article, i)\n",
    "        new_ds.append({\"article\": x, \"article_text\": article, \"highlights\": higlights})\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "        \n",
    "def load_file(filename):  \n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedd_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-ae4c6b6c1a74>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtest_ds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0membedd_ds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_tds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0msave_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"test.pkl\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mval_ds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0membedd_ds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_tds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0msave_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"val.pkl\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'embedd_ds' is not defined"
     ]
    }
   ],
   "source": [
    "test_ds = embedd_ds(test_tds)\n",
    "save_file(test_ds, \"test.pkl\")\n",
    "\n",
    "val_ds = embedd_ds(val_tds)\n",
    "save_file(val_ds, \"val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = embedd_ds(train_tds)\n",
    "# save_file(train_ds, \"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13368 [list([array([ 0.00387947, -0.6366544 ,  0.21970668, ...,  0.30173776,\n",
      "        0.43474218,  0.42230588], dtype=float32), array([-0.23565385, -0.659557  ,  0.55651265, ...,  0.33840343,\n",
      "        0.25438353,  0.26119417], dtype=float32), array([-0.31036785, -0.07266833,  0.42479414, ...,  0.11688844,\n",
      "        0.4397149 ,  0.7277298 ], dtype=float32), array([ 0.09201267, -0.44499242,  0.3602318 , ...,  0.15546696,\n",
      "        0.27280316,  0.11705698], dtype=float32), array([-0.2923365 , -0.36915168,  0.52738214, ...,  0.0649092 ,\n",
      "        0.25962955,  0.14565222], dtype=float32), array([ 0.0709545 , -0.17259236,  0.8320757 , ..., -0.06098118,\n",
      "        0.24208799,  0.09772751], dtype=float32), array([-0.11589033,  0.35133335,  0.81095463, ...,  0.03234402,\n",
      "        0.34122986,  0.13773124], dtype=float32), array([ 0.0746538 , -0.13619456,  0.7168364 , ...,  0.0407102 ,\n",
      "        0.2888261 ,  0.3364408 ], dtype=float32), array([-0.14699861, -0.35566977,  0.88771176, ...,  0.03221212,\n",
      "        0.01457863,  0.13869776], dtype=float32), array([-0.22204223, -0.6054286 ,  0.78574795, ...,  0.09142061,\n",
      "        0.16640931,  0.48191297], dtype=float32), array([ 0.21558289, -0.78932345,  0.49542183, ..., -0.11635831,\n",
      "        0.3304542 ,  0.5526388 ], dtype=float32), array([-0.13048227, -0.4027023 ,  0.57894194, ...,  0.03963578,\n",
      "        0.40740883,  0.912753  ], dtype=float32), array([-0.06654014, -0.3630797 ,  0.8600646 , ...,  0.18303427,\n",
      "        0.1893257 ,  0.3632384 ], dtype=float32)])\n",
      " \"sally forrest, an actress-dancer who graced the silver screen throughout the 40s and '50s in mgm musicals and films such as the 1956 noir while the city sleeps died on march 15 at her home in beverly hills, california. forrest, whose birth name was katherine feeney, was 86 and had long battled cancer. her publicist, judith goffin, announced the news thursday. scroll down for video . actress: sally forrest was in the 1951 ida lupino-directed film 'hard, fast and beautiful' (left) and the 1956 fritz lang movie 'while the city sleeps' a san diego native, forrest became a protege of hollywood trailblazer ida lupino, who cast her in starring roles in films including the critical and commercial success not wanted, never fear and hard, fast and beautiful. some of forrests other film credits included bannerline, son of sinbad, and excuse my dust, according to her imdb\\xa0page. the page also indicates forrest was in multiple climax! and rawhide television episodes. forrest appeared as herself in an episode of the ed sullivan show and three episodes of the dinah shore chevy show, her imdb page says. she also starred in a broadway production of the seven year itch. city news service reported that other stage credits included as you like it, no, no, nanette and damn yankees. forrest married writer-producer milo frank in 1951. he died in 2004. she is survived by her niece, sharon durham, and nephews, michael and mark feeney. career: a san diego native, forrest became a protege of hollywood trailblazer ida lupino, who cast her in starring roles in films .\"\n",
      " 'sally forrest, an actress-dancer who graced the silver screen throughout the 40s and 50s in mgm musicals and films died on march 15 .\\nforrest, whose birth name was katherine feeney, had long battled cancer .\\na san diego native, forrest became a protege of hollywood trailblazer ida lupino, who cast her in starring roles in films .']\n"
     ]
    }
   ],
   "source": [
    "train_raw = load_file(\"val.pkl\")\n",
    "test_raw = load_file(\"test.pkl\")\n",
    "print(len(train), train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.00387947, -0.6366544 ,  0.21970668, ...,  0.30173776,\n",
      "         0.43474218,  0.42230588],\n",
      "       [-0.23565385, -0.659557  ,  0.55651265, ...,  0.33840343,\n",
      "         0.25438353,  0.26119417],\n",
      "       [-0.31036785, -0.07266833,  0.42479414, ...,  0.11688844,\n",
      "         0.4397149 ,  0.7277298 ],\n",
      "       ...,\n",
      "       [ 0.21558289, -0.78932345,  0.49542183, ..., -0.11635831,\n",
      "         0.3304542 ,  0.5526388 ],\n",
      "       [-0.13048227, -0.4027023 ,  0.57894194, ...,  0.03963578,\n",
      "         0.40740883,  0.912753  ],\n",
      "       [-0.06654014, -0.3630797 ,  0.8600646 , ...,  0.18303427,\n",
      "         0.1893257 ,  0.3632384 ]], dtype=float32), \"sally forrest, an actress-dancer who graced the silver screen throughout the 40s and '50s in mgm musicals and films such as the 1956 noir while the city sleeps died on march 15 at her home in beverly hills, california. forrest, whose birth name was katherine feeney, was 86 and had long battled cancer. her publicist, judith goffin, announced the news thursday. scroll down for video . actress: sally forrest was in the 1951 ida lupino-directed film 'hard, fast and beautiful' (left) and the 1956 fritz lang movie 'while the city sleeps' a san diego native, forrest became a protege of hollywood trailblazer ida lupino, who cast her in starring roles in films including the critical and commercial success not wanted, never fear and hard, fast and beautiful. some of forrests other film credits included bannerline, son of sinbad, and excuse my dust, according to her imdb\\xa0page. the page also indicates forrest was in multiple climax! and rawhide television episodes. forrest appeared as herself in an episode of the ed sullivan show and three episodes of the dinah shore chevy show, her imdb page says. she also starred in a broadway production of the seven year itch. city news service reported that other stage credits included as you like it, no, no, nanette and damn yankees. forrest married writer-producer milo frank in 1951. he died in 2004. she is survived by her niece, sharon durham, and nephews, michael and mark feeney. career: a san diego native, forrest became a protege of hollywood trailblazer ida lupino, who cast her in starring roles in films .\", 'sally forrest, an actress-dancer who graced the silver screen throughout the 40s and 50s in mgm musicals and films died on march 15 .\\nforrest, whose birth name was katherine feeney, had long battled cancer .\\na san diego native, forrest became a protege of hollywood trailblazer ida lupino, who cast her in starring roles in films .']\n",
      "{}\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "def convert_to_np(ds):\n",
    "    list_ds = []\n",
    "    for item in ds:\n",
    "        x = [np.array(item['article']), item['article_text'], item['highlights']]\n",
    "        print(x)\n",
    "        list_ds.append(np.array(x))\n",
    "        sp_model = spm.SentencePieceProcessor()\n",
    "        vocab = {sp_model.IdToPiece(i): i for i\n",
    "                    in range(sp_model.GetPieceSize())}\n",
    "        print(vocab)\n",
    "        break\n",
    "    return np.array(list_ds)\n",
    "\n",
    "train = convert_to_np(train_raw)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = convert_to_np(test_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train[:,0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.x_embed = torch.from_numpy(ds[:,0])\n",
    "        self.x_text = torch.from_numpy(ds[:,1])\n",
    "        self.y = torch.from_numpy(ds[:,2])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_embed = self.x_embed[index]\n",
    "        x_text = self.x_text[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            x_embed = self.transform(x_embed)\n",
    "            x_text = self.transform(x_text)\n",
    "            \n",
    "        return x_embed, x_text, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_embed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = MyDataset(train)\n",
    "test = MyDataset(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install sentencepiece\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.00387947, -0.6366544 ,  0.21970668, ...,  0.30173776,\n",
      "         0.43474218,  0.42230588],\n",
      "       [-0.23565385, -0.659557  ,  0.55651265, ...,  0.33840343,\n",
      "         0.25438353,  0.26119417],\n",
      "       [-0.31036785, -0.07266833,  0.42479414, ...,  0.11688844,\n",
      "         0.4397149 ,  0.7277298 ],\n",
      "       ...,\n",
      "       [ 0.21558289, -0.78932345,  0.49542183, ..., -0.11635831,\n",
      "         0.3304542 ,  0.5526388 ],\n",
      "       [-0.13048227, -0.4027023 ,  0.57894194, ...,  0.03963578,\n",
      "         0.40740883,  0.912753  ],\n",
      "       [-0.06654014, -0.3630797 ,  0.8600646 , ...,  0.18303427,\n",
      "         0.1893257 ,  0.3632384 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.x_embed = torch.from_numpy(ds[:,0])\n",
    "        self.x_text = torch.from_numpy(ds[:,1])\n",
    "        self.y = torch.from_numpy(ds[:,2])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_embed = self.x_embed[index]\n",
    "        x_text = self.x_text[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            x_embed = self.transform(x_embed)\n",
    "            x_text = self.transform(x_text)\n",
    "            \n",
    "        return x_embed, x_text, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-6e3b659401a1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMyDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMyDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-34-6270ad909016>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, ds, transform)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mMyDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx_embed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "train = MyDataset(train)\n",
    "test = MyDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/yannik/.local/lib/python3.7/site-packages (0.1.85)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}